# **体外培养神经网络闭环训练调控及智能行为增强研究**



### 1 文献调研相关

1. [计时是神经网络的内在属性：来自体内和体外实验的证据 |英国皇家学会哲学汇刊 B：生物科学](https://royalsocietypublishing.org/doi/full/10.1098/rstb.2012.0460)

> 生物科学 二区 这篇文章中有体外的描述 14年

看一下盲源分离技术，从这个角度看一下细胞究竟是如何理解外部刺激的，

2. [Experimental validation of the free-energy principle with in vitro neural networks | Nature Communications](https://www.nature.com/articles/s41467-023-40141-z)

> NC 一区 自由能原理的描述
>
> 可以看一下都有做什么实验，分析了什么数据？

3. [A primer on in vitro biological neural networks | OpenReview](https://openreview.net/forum?id=RdFI2ogt1j)

> 体外生物神经网络入门 

4. [3D 工程神经元培养物的网络动力学：体外电生理学的新实验模型 |科学报告](https://www.nature.com/articles/srep05489)

> 一篇3D神经网络的文章，可能与帮助

5. [In Vitro Studies of Neuronal Networks and Synaptic Plasticity in Invertebrates and in Mammals Using Multielectrode Arrays - Massobrio - 2015 - Neural Plasticity - Wiley Online Library](https://onlinelibrary.wiley.com/doi/full/10.1155/2015/196195)

> 四区 神经元图可参考

6. https://www.x-mol.com/paper/1697107062972764160/t?adv=

> **[体外神经元网络内的结构化信息呈现过程中出现关键动态](https://www.x-mol.com/paperRedirect/1697107062972764160)**
>
> 我们证明，当神经网络接收到与任务相关的结构化感官输入时，就会出现临界动态，从而将系统重新组织到接近临界状态。

7. Critical dynamics arise during structured information presentation within embodied in vitro neuronal networks.

   > NC 2023
   >
   > 实验结果表明，当神经网络接收到与任务相关的结构化感官输入时，系统会自组织到一个**接近临界的状态**。
   >
   > 神经系统可能在临界状态下优化信息传递和处理，临界状态的**典型特征**
   >
   > 因此，有研究者提出临界性可能是皮层网络自组织的一种设定点。然而，一些理论研究指出，**临界状态只在复杂的认知任务中带来益处**，而对于静息状态的网络而言，这种动态性并不重要。
   >
   > 此外，健康成人在执行工作记忆和认知任务时，响应时间的波动也呈现幂律分布，这进一步支持了临界状态对认知活动的潜在重要性。
   >
   > 大脑中的神经网络是否在自发活动时就表现出临界状态，还是只有在处理结构化信息（如认知任务）时才出现？

### 2 研究背景与现状

**1 现有体外BNNs与机器结合系统的局限性 + 缺乏智能行为的挖掘**

**生物脑-机混合智能**结合了生物大脑和机器智能各自的优势，未来有望于借助生物智能对复杂环境的感知能力，达到强于机器智能的精确性和高效性。目前，体外培养的神经网络（BNNs）已被广泛应用于生机混合系统中，例如机器人的基本运动控制和路径规划等场景。然而，大多数系统的研究重心放在**基础的神经-机器接口搭建**上，例如实现神经网络控制简单的运动指令或反射性行为，而忽略了对**智能行为提升**的探索。现有的研究仅限于**实现信号通路连接**，建立**基本的感知-反馈通路**。这些系统在实际应用中往往只满足“完成任务”的要求，而非“优化任务表现”，因此难以适应复杂的环境或具有更高要求的任务场景。缺乏对BNNs在动态任务中的表现及其智能提升的深入挖掘

- **缺乏系统性训练手段**：现有训练方法往往不能长期有效，难以在BNNs中实现高度稳定的智能行为。
- **缺乏对刺激反馈的优化**：由于神经调节剂等生理条件在体外环境中不易复现，传统的随机刺激难以帮助BNNs逐步塑造稳定的智能行为。

**2 研究意义**

BNNs 天然具备自组织和突触可塑性等特性，在理论上有可能实现自适应和学习。然而，现有的BNNs+机器系统还没有充分利用这些特性，仅作为简单的“信号转换器”来执行机械性的任务，忽略了BNNs在适应性和决策性任务中的潜在优势。为此，研究如何在现有BNNs与机器结合系统中引入智能行为的提升机制成为了一个具有重大意义的方向。

**3 研究必要性**

BNNs 天然具备自组织和突触可塑性等特性，在理论上有可能实现自适应和学习。针对目前仅限于“搭建通路”的生机混合系统，我们的研究将进一步探索如何通过**闭环训练和自适应（动态）反馈**来**提高BNNs在控制任务中的智能表现**，推动生机混合系统从基础控制阶段迈向智能行为阶段。智能行为的提升不仅有助于增强系统在复杂任务环境中的表现，也能为未来更高层次的生物控制应用奠定基础。

**4 研究目的 or 研究内容**

- 基于闭环动态反馈训练策略，实现智能行为的提升。

- 闭环策略训练下需要真正明确的应该是训练后，网络出现了什么状态使得任务执行效率更高（如何评价网络状态，有什么指标，pong中有一个临界态评估），建立一套训练效果评估体系。

  > **临界状态**是一种动态平衡状态，网络在此状态下最有效地传输和存储信息，同时能够优化响应的灵敏度和适应性。大脑的临界状态通常出现在需要处理复杂信息或做出决策的任务中，这种状态能提升其认知和学习能力。
  >
  > **且临界状态更可能由结构化信息输入或环境反馈驱动**，使神经网络调整到最佳信息处理状态。

### 3 技术路线

#### 1. **探究基础的刺激模式以及网络的基础响应模式 - 	监督学习方法去探究**

> 目前已经探究了基础的单点刺激的诱发响应强度变化，但是这种刺激应用于闭环等其他任务时，会因为过于频繁而导致细胞响应呈现不规律的变化，因此此处探究的是可用于闭环高频的基础刺激模式，以便用作刺激池中刺激模式的生成。

- **依赖点**：首先了解BNNs对基础刺激的自然响应模式。可以通过初步实验观察其自发活动、对不同频率和强度的刺激响应（目前的梯度实验基本是单点刺激，考虑MEA电流刺激的方式，闭环的频率疲劳问题，这个刺激如何设置才能最大程度的影响BNNs的网络变化）等，这样可以为训练策略的基线设定提供数据。

- **设计依据**：基于这些基础反应模式，以及综述中提到的非线性计算能，保持训练的稳定性。

- **做不同刺激参数的组合实验**

  > 设计多样化的训练策略需要测试不同的刺激参数组合（如强度、频率、空间位置等），以**筛选出适合的刺激模式**。
  >
  > 通过组合实验来确定BNNs在不同参数下的最佳反应模式。这样的组合测试希望**找到一套系统的刺激方案**，用于在训练中优化网络表现，这个系统的刺激方案会用作之后刺激池的准备。
  >
  > 参考pingpong奖惩反馈在设置的时候是一个固定的，基于预实验选择的幅值频率固定的刺激，只是施加范围有时随机。

#### 2. **编解码码策略的设计**

- **依赖点**：编码策略是训练的核心，需要确保编码后的刺激能够最大程度的反应现实信息，并且能够被BNNs感知，这个感知在于温和感知，例如小车控制的频率过高，任务的复杂度，其实一定程度上很难评估真实的网络状态，至少让细胞保证温和可正常响应的状态
- 这部分虽然课题三中并不是我们完成，但是这部分很大程度也会影响训练效果，至少需要有一套基本的可供训练测试用的编解码策略。

#### 3. **设定特定行为目标与反馈机制**

- **依赖点**：明确BNNs在训练中的具体行为目标，目标越具体，训练策略的设计就越具针对性。
- **设计依据**：监督学习中到底该基于什么目标去设定响应强度阈值，强化学习基于什么任务进行奖惩，何时进行奖惩，什么刺激形式（电，光，化学）。

#### 4. **考量神经网络的各个突触可塑性机制以及网络结构进行训练设计**

- **依赖点**：突触可塑性（如长期增强或短期突触调节）这种生理基础。设计训练策略时，需要充分利用和激发BNNs的突触可塑性，以便使其逐步形成稳定的学习模式。

- **设计依据**：可以利用重复刺激和周期性训练，如课题三中提到的时间依赖刺激，**使BNNs的突触发生持久性调整**。通过观察网络的突触变化来调节训练强度或频率，以促进突触在学习过程中逐步加强。

  > 这个突触可塑性，突触的变化如何观测？人工构建的网络结构是否能真实反映细胞级的突触变化？

#### 5. **总体是基于闭环系统的动态调整**

- **依赖点**：BNNs的状态是动态变化的，因此训练策略也应具备灵活性，能够根据网络的实时反馈动态调整训练参数。
- **设计依据**：在实验中观察BNNs的状态变化，动态调整刺激的类型、频率和强度，位置。目前的闭环系统基本完备，对该平台复现基本可开发的程度后，进行闭环任务的探究。

有效的训练策略设计是建立在以上提到的几点，可能接触到后续的这种依赖有助于根据BNNs的具体特性和实验目标设计有针对性的训练方法，从而在最大程度上提升BNNs的智能行为表现。

- 具体路线

第一步我其实想基于闭环反馈式的训练，使得网络对于特定的刺激有预期的响应，做一定程度的调控。

> 刺激的范围是不是有要求，考虑到MEA的电流刺激方式，这个范围的临界态是什么？

第二步在未明确机制的情况的下，基于自适应奖惩做任务训练的尝试

......

### 3.1 训练方法

想要基于两种方式对体外神经网络（BNNs）尝试进行训练，看是否能够达到预期效果

监督学习 + 强化学习训练BNNS，训练方式和反馈机制：

#### 1. **监督学习 (Supervised Learning)**

- **训练方式**：在监督学习中，BNNs的每次输出行为都与预期的响应进行比较（即目标输出）。训练过程中，会提供一个明确的响应强度阈值，如果BNNs的神经活动被逐步调整，实际输出与目标之间的误差将随着训练而减少。

- **反馈机制**：监督学习依赖于精确的误差反馈。这个误差反馈目前基于什么来界定误差？，设定误差触发负反馈，让网络逐步调整其行为以符合目标方向。。

  > 前期的不涉及复杂任务，仅仅根据响应强度等简单可测量的信息进行误差反馈，后期如果任务是让BNNs控制微纳鱼朝一个特定方向游动，每次偏离目标方向角度设定误差都会触发负反馈，让网络逐步调整其行为以符合目标方向。
  >
  > **实现倒是不容易，需要把闭环数据分析功能从 MetaBOC中抽离出来，仅用作闭环监督学习实验，不然使用 metaBOC 因功能太多冗余导致，没有侧重点，实验用起来也麻烦，还得开发针对这个实验的功能**

#### 2. **强化学习 (Reinforcement Learning)**

- **训练方式**：在强化学习中，BNNs不会有固定的目标输出，而是根据动作的结果进行学习。训练过程会给出奖励或惩罚刺激，以引导BNNs调整行为来增加正反馈（奖励）并减少负反馈（惩罚）。

- **反馈机制**：强化学习的反馈基于行为的**整体结果**，而不是即时的误差。

  > 例如，在避障任务中，当BNNs成功避开障碍物时，它们会收到正反馈；如果与障碍物发生碰撞，则会收到负反馈。通过不断试错，BNNs逐渐学会避开障碍，理想希望是习得一条路线。
  >
  > 其实我并不想基于小车做强化学习的尝试，小车的不确定性太高，而且训练难度会更大，目前小车的多轮次实验也并没有出现明显的效果提升，而是因为刺激造成的相应通道信号波动呈现一定规律的变化，进而使得控制效果提升。我想复现一个类似pong游戏的简单游戏，**“有明确的正确与错误行为”**（鉴于先前游戏开发实习经历以及在脑虎实习的经历，这个的开发成本其实不高，可能也就一周左右，最长算到一个月时间也足够用于之后的实验），而且训练成本相较于小车避障任务也将更容易，将该游戏接入体外BNNs的训练中，由此构建基于游戏任务的反馈机制（奖惩将更容易设置）。

- **要点**：强化学习适合**没有固定答案、需要探索**的场景，特别是面对不确定环境时，通过奖励机制逐渐优化策略。

#### 3. **自适应奖惩刺激方法**

- **训练方式**：自适应奖惩刺激方法是一种**较灵活且迭代**的策略，不直接依赖于对BNNs内部机制的全面理解。通过反馈**逐步调整刺激选择概率**，在机制不完全明了的情况下进行训练。允许BNNs在无须详细阐明内部运作机制的情况下逐步优化功能。

  

**表格对比，PPT一页**

| 监督学习                 | 特点         | 强化学习                           |
| ------------------------ | ------------ | ---------------------------------- |
| 明确的目标值和误差反馈   | **反馈类型** | 奖励或惩罚反馈                     |
| 减少输出与目标值的差距   | **目标**     | 增加长期奖励，优化行为策略         |
| 定向控制、特定行为的训练 | **适用任务** | 探索性任务、不确定环境中的行为优化 |
| 学习与预期值的差距       | **学习过程** | 通过试错法优化长期行为表现         |

- **监督学习**：BNNs被要求控制微纳鱼游向指定方向，每次偏离都会触发负反馈，BNNs逐步校正输出，以符合预期方向。
- **强化学习**：BNNs在障碍物周围探索最优路径，每次成功避开障碍物获得奖励，随着试错次数增加，BNNs逐步学会自适应避障策略。



### 4 研究困难： 

这一领域面临的主要挑战包括：

- **缺乏多种神经调节因子**：由于体外BNN仅由神经元组成，缺乏多种神经调节因子的参与，因此将动物的训练方法移植到BNN的训练中变得困难，这增加了训练的难度。
- **生物智能的局限性**：虽然猴子可以被训练成骑自行车，但对于需要更高层次思维过程的任务（如下围棋），体外BNN很难达到类似水平。
- **稳定的通信**：稳定的通信是训练的基础，但由于我们尚未明确BNN中神经元间和网络内通信的正确编码机制，因此找到合适的通信方式仍然具有挑战性。



### **6 Idea Points**

1. 智能的聚焦点可能还得回到学习记忆机制上？

   BNNs的响应模式

   刺激信号基于突触结构的传递？进而引发特定局部的反应模式？这种突触结构是否会被破坏呢？

   如何调整突触的强度？

> - 体外神经网络只有神经元通过突触连接构成的网络结构，所以智能聚焦于网络，探索一下关于训练前后网络的临界态有什么变化，网络变化的原因是什么？突触连接的变化？那么突触会不会被破坏？对刺激的要求会有限定。
> - 体外BNNs尽管没有完整的调节机制，但它们仍然能够展示出神经元间的**自组织行为**、**突触可塑性**和某些程度的学习与记忆。通过研究这些简单的神经网络，可以揭示出**神经元间如何进行信息传递**、**如何通过突触连接形成复杂的动力学模式**。这是理解更大规模神经系统的基础。

3. 如果在解码方法中加入神经网络算法，是一种创新吗？

   解码策略上还能改嘛？只能解码spike？

> 目前的分析可能还得聚焦于 spike 分析，具体的响应模式可以引入算法进行一下分类划分。

4. **神经动力学？**

> 这个问题需要有些时间好好了解一下，因为这个可能是之后闭环训练的基础。
>
> 一直提动力学特征，这个特征到底是什么

5. 我想要从形态结构上去研究在训练前后的变化？

> 形态该基于什么方法去研究？

6. 刺激调控方式其实也可以优化

   > 光刺激（例如通过光遗传学技术）可以更精确地调控特定神经元的活动，从而引发不同的反应模式。



### 7  Questions



**动物的神经调节机制是什么？**

**机器学习的控制模型**：体外BNNs训练通常通过机器学习方法学习合适的刺激序列，以控制神经网络的行为。这包括设计反馈机制，在检测到不期望的网络活动时施加特定的反馈信号，以引导网络向所需状态收敛

**不可预测的电刺激**：当BNNs在任务中表现不佳时，施加随机或不可预测的电刺激作为负反馈。例如，在训练BNNs玩“Pong”游戏时，每当“球拍”未能接住球，研究人员会施加随机的电刺激，促使BNNs调整其活动以避免再次出现错误。 

**特定模式的电刺激**：根据BNNs的表现，施加预先设定的电刺激模式作为负反馈。这种方法需要在实验前设计好负反馈刺激的模式，并在BNNs表现不符合预期时施加，以引导其朝正确方向调整。

**化学刺激**：除了电刺激，还可能使用化学物质（如神经递质或其类似物）来施加负反馈。通过改变培养基中化学物质的浓度，影响BNNs的活动，从而实现负反馈效果。



### 8 创新点

> 除了训练方法之外的可尝试的创新

### 1. **特定任务依赖的网络结构重组**

**创新点**：研究如何通过任务导向的训练，诱导网络形成更高效的拓扑结构（探究临界态是否能够评估效果，并看是否能找到类似临界态的评估方法）。与仅关注神经元活动不同，可以研究在特定任务下网络拓扑如何自我调整，逐步发展出高度分化的模块化结构，从而提升复杂任务的执行效率。

**实现方法**：设计任务情境，观察特定结构（例如模块化的子网络）是否在任务执行中被激活或形成。如果发现特定任务依赖某些网络模块，则可以进一步通过有针对性的训练策略来增强这些模块的功能（60位点够吗）。

设计的这个任务要足够简单（太复杂感觉变量太多，困难系数更高，尤其做小车真实的控制训练上），侧重点放在网络变化上，走之前课题四评估体系。

怎么评价网络拓扑？还是从60位点建立拓扑结构？

### 2. **探索从“噪声”中提取信息的能力**

**创新点**：探索体外神经网络在高噪声环境下的智能行为表现。训练神经网络从噪声中提取有用信号，是否可以增强其对复杂环境的适应能力。

**实现方法**：在闭环训练中引入高背景噪声，观测网络如何过滤无关噪声并专注于任务相关的信号。通过调整噪声水平，逐步提升网络对低信噪比环境的适应性。

### 3. **体外BNNs的“自主”信息处理能力 智能基础**

**创新点**：研究神经网络在完全自主的情况下能否生成智能行为，减少外部干预。例如，在无任务或无明确刺激的条件下，观察其自发的突触重组和模式形成。

**实现方法**：相当于更换任务场景，创建一个无特定任务的闭环环境，通过监测自发活动的变化，分析网络是否会自发形成类似于“探索”或“试错”的行为模式。探索其内部是否存在一种“自我组织”的智能基础。

**Pingpong：**

**条件1（刺激反馈）**：当神经网络表现符合期望时给予可预测的刺激，表现不符合期望时则施加不可预测的刺激。

**条件2（静默反馈）**：与刺激反馈条件不同，静默反馈不提供任何刺激，而是用等长时间段的静默代替。

**条件3（无反馈）**：在这种条件下，改变了游戏环境，移除了乒乓板“错失”球的可能性。乒乓板未能拦截到球时，球会弹回并继续游戏，而不是触发游戏重置。

**实验结果**：

- **静默反馈条件**：静默条件下，神经网络在任务中的表现显著提升，接近临界状态的特征更加明显。相比于刺激和无反馈条件，静默条件下的临界偏离系数（DCC）显著更低（p < 0.005）。
- **刺激反馈与无反馈的对比**：在游戏表现（H/M比例 命中/未命中）方面，刺激反馈和静默反馈条件都显著优于无反馈条件（p < 0.0005和p < 0.005）。
- **无反馈条件**：尽管无反馈条件下网络也表现出一定的临界动态特征，但游戏表现明显下降，与静息状态相比没有显著差异（p = 0.085）。

![image-20241106140843488](C:\Users\Yundid\AppData\Roaming\Typora\typora-user-images\image-20241106140843488.png)





### 1. **是否可尝试 微观层面的智能行为增强：动态突触调控**

**创新点**：在神经网络的局部突触连接层面，通过精细的闭环控制实现特定突触连接的塑性变化。例如，不仅观察整个网络的活动，还要对关键突触连接进行个性化的刺激调节，培养出特定的功能性连接。这可以涉及动态监测和调节突触强度，以便在执行特定任务时，增强某些特定通路的活性。

**实现方法**：在闭环反馈过程中，监测局部突触反应，并通过自适应算法将“奖励”刺激更集中地施加在关键突触区域，观察这种局部调控对整体智能行为的影响。

**能实验中观测细胞的状态变化吗？**

### 2. **是否可尝试 引入多模态输入**

**创新点**：模拟更真实的多感官输入，以探索不同感官信息融合在智能行为中的作用。多数研究仅采用单一的电刺激或单一类型的输入，而实际的生物系统往往依赖多种感官信息的协同。

**实现方法**：设计多模式输入通道，例如将光、电、化学刺激结合在一个闭环训练系统中。

**光刺激**可以精确地针对特定的神经元群体或区域进行**特异性刺激**

**电刺激**可以有效激活更大范围的神经元或网络结构，但是空间精确度低。

1. **光电结合**是否可以充分利用光刺激的**高选择性**和电刺激的**能量效率**。例如，通过光来精确选择目标区域，再配合电刺激激发整个神经网络，从而实现更灵活、精细的控制。
2. 光电结合，是否可以在输入上编码不同的信息。例如，使用电刺激传递主要的时序信号（如快速的脉冲频率，固定幅值），同时用光信号提供更复杂的调制信息（如光的强度或频率），以便更细腻地影响神经元群体的行为。
3. 通过交替使用光和电刺激，是否可以减少单一刺激造成的“疲劳”，保持神经网络的活跃度？

**这个光电平台搭建就够呛了，保证不了精确的双模块同时刺激输入**

### 3. **是否可尝试 基于网络“疲劳”状态的学习与记忆增强**

**创新点**：研究体外神经网络在“疲劳”状态下的学习与记忆能力。当前研究大多集中在网络的活跃状态，而“疲劳”状态下的神经活动可能会反映出一些特殊的学习机制，例如巩固记忆或优化资源分配。

**实现方法**：在训练过程中，故意引入高频刺激，让网络进入疲劳状态，并观察这种状态下的突触可塑性变化。分析网络是否会在疲劳状态中优先形成更稳固的长时程突触连接，从而提升任务记忆的持久性。

### 4. 引入 ANN 作解码分析？

### 5. 是否需要结合模块化，如何结合模块化？MEA60 精度是否在模块化后，执行复杂效率的性能会降低？





### Critical dynamics arise during structured information presentation within embodied in vitro neuronal networks.

> NC 2023
>
> 实验结果表明，当神经网络接收到与任务相关的结构化感官输入时，系统会自组织到一个**接近临界的状态**。
>
> 神经系统可能在临界状态下优化信息传递和处理。
>
> 临界状态的**典型特征**
>
> 因此，有研究者提出临界性可能是皮层网络自组织的一种设定点。然而，一些理论研究指出，**临界状态只在复杂的认知任务中带来益处**，而对于静息状态的网络而言，这种动态性并不重要。
>
> 此外，健康成人在执行工作记忆和认知任务时，响应时间的波动也呈现幂律分布，这进一步支持了临界状态对认知活动的潜在重要性。
>
> 大脑中的神经网络是否在自发活动时就表现出临界状态，还是只有在处理结构化信息（如认知任务）时才出现？

1 核心在于**通过检测神经网络在不同条件下的临界性指标，验证其是否接近临界状态**。这部分实验表明，当体外皮层网络在任务状态下，其活动更接近临界状态，且临界性指标与任务表现密切相关。这表明，临界动态可能与神经网络的任务处理能力直接相关，接近临界状态的网络在执行任务时表现更为优越。

2 在任务状态下，运动和感官神经元子群同样表现出接近临界系统的特性。

> 这部分研究表明，在任务状态下，运动和感官子群神经元同样会继承整体神经网络的临界特征。这意味着，即使在特定子群中，神经元的活动在任务中也呈现出接近临界状态的特征，从而提升了任务表现。

3 实验结果表明，**反馈机制在任务中的重要性**，尤其是在临界状态下能进一步提升神经网络的信息处理能力。反馈允许神经网络根据行为结果不断调整和优化，缺乏反馈会让网络失去学习和自我优化的驱动力，因此即使网络接近临界状态，虽然接近临界状态有助于信息处理，但在缺乏关于先前行为后果的反馈信息时，单纯的临界性不足以使神经网络实现学习和记忆。

4 简而言之，这段内容表明不同神经元培养物在静息状态下的爆发模式会影响其临界性指标，这种模式的多样性提示了使用多种神经元文化进行实验的必要性，以获得全面的结果。



### 其他文章启发

早期实验包括对动物神经元的多电极刺激，通过混沌理论的应用来管理**神经元的混沌电响应**，**并尝试从中得出简单的计算结果**。





# 储备池计算相关

#### **一、储备池计算（Reservoir Computing, RC）的起源**

储备池计算（RC）最早起源于 **2000 年**，由两种不同但类似的神经网络模型分别提出：**Echo State Network (ESN)** 和 **Liquid State Machine (LSM)**。这两种模型提出了 **不同于传统循环神经网络（RNN）** 的新思路，旨在解决 RNN 中的 **梯度消失和梯度爆炸问题**，并简化训练过程。

- **传统循环神经网络的挑战**：

  - 训练循环神经网络时需要对整个网络进行反向传播，这往往会导致 **梯度消失问题**，尤其是在处理长时间序列时，模型难以捕捉到输入信号中的长时记忆。
  - 训练过程复杂，计算开销大，不适合实时任务。

- **RC 的核心思想**：

  - **Cover 定理** ：在高维空间中，随机投影的数据点更容易被线性分离。储备池计算通过非线性映射和高维投影，能够 **揭示时间序列中隐藏的模式和特性**。

  - 储备池的主要任务是对输入信号进行 **非线性动态映射**，将其转换为 **高维动态状态**，从而捕捉到输入信号中的**复杂模式和时间特征**。

    > RC 提出了一种新颖的架构设计：将循环网络分为 **储备池（Reservoir）** 和 **输出层（Readout Layer）**。储备池的内部连接是**随机初始化并保持权重不变**的，只训练一个简单的 **线性输出层**（readout layer），极大简化了计算复杂度。
  
  ![image-20241113222740160](C:\Users\Yundid\AppData\Roaming\Typora\typora-user-images\image-20241113222740160.png)

#### **二、储备池计算的理论基础**

储备池计算的理论基础来源于 **高维动态系统** 和 **统计学习理论**，主要包含以下几部分：

1. **高维映射（High-dimensional Mapping）**：
   - 输入信号经过储备池的随机连接后，被映射到一个高维空间。高维空间中，原本低维空间中复杂的非线性模式会被展开，使得后续的线性分类或回归变得更加容易。
   - 这种高维映射类似于 **支持向量机（SVM）** 中的核技巧，通过非线性映射将数据变得线性可分。
2. **回声状态特性（Echo State Property, ESP）**：
   - 在 **Echo State Network (ESN)** 中，储备池需要满足回声状态特性，即输入信号的影响会随着时间逐渐衰减，不会无限传播。这保证了储备池状态的稳定性，使其能够对当前输入和近期输入进行短时记忆。
3. **液体状态特性（Liquid State Property, LSP）**：
   - 在 **Liquid State Machine (LSM)** 中，储备池由尖峰神经网络（Spiking Neural Network, SNN）构成，模拟大脑中神经元的尖峰发放特性。输入信号进入储备池后，会像石头丢入水中一样，引发一系列复杂的动态响应，这种涌现的瞬态状态记录了输入信号的时间特征。

- **SVM vs RC**

| 特性             | 支持向量机（SVM）                  | 储备池计算（RC）                         |
| ---------------- | ---------------------------------- | ---------------------------------------- |
| **高维映射方式** | 显式或隐式使用核函数进行映射       | 通过储备池的随机连接和非线性响应实现映射 |
| **映射性质**     | 静态映射，无时间动态特性           | 动态映射，具有时间记忆和非线性特性       |
| **训练方式**     | 二次规划求解，计算复杂度高         | 只训练输出层，计算复杂度低               |
| **适用场景**     | 静态分类任务，如图像识别、文本分类 | 时序任务，如语音识别、时间序列预测       |
| **理论基础**     | 最大间隔原则、Mercer 定理          | 回声状态特性、液体状态特性               |

- **Echo State Network (ESN) Vs Liquid State Machine (LSM) **

| 特性           | Echo State Network (ESN)   | Liquid State Machine (LSM)         |
| -------------- | -------------------------- | ---------------------------------- |
| **网络类型**   | 循环神经网络 (RNN)         | 尖峰神经网络 (SNN)                 |
| **储备池权重** | 随机初始化，不参与训练     | 随机初始化，不参与训练             |
| **储备池状态** | 连续值（神经元激活值）     | 离散尖峰发放序列                   |
| **动态特性**   | 回声状态特性，具有短时记忆 | 液体状态特性，尖峰活动捕捉时间特征 |
| **应用场景**   | 时间序列预测、动态系统建模 | 神经科学建模、语音处理、机器人控制 |
| **计算效率**   | 计算相对简单               | 计算复杂度高，需要模拟尖峰发放     |

4. 基础概念

- **非线性空间** 指的是输入数据在经过**非线性变换（如激活函数）后**，不能简单地用线性关系表示。

- **线性（Linear）** 一般表示的是一种 **简单的关系**，这种关系满足 **叠加性** 和 **比例性** 两个特性。在几何上，线性关系通常表示为一条 **直线**，在高维空间中，线性关系可以表示为 **超平面**。

- **线性可分（Linearly Separable）** 是指在给定的数据集中，不同类别的数据点可以被一条 **直线**（在二维空间中）或一个 **超平面**（在高维空间中）完全分开。与数据本身是否具有线性属性无关。

- **动态网络（Dynamic Network）** 是指一种在 **时间上不断变化** 的网络结构，其输出不仅依赖于当前的输入，还依赖于过去的输入和网络的历史状态。简而言之，动态网络具备 **记忆** 和 **反馈** 特性。与静态网络不同，动态网络具有 **时间依赖性**，可以在 **时序数据** 中学习和捕捉时间相关的模式。

> 摆钟（动态系统），当你推一下摆钟时，它会开始摆动。这种摆动不仅取决于你推的力度（输入），还受到摆钟之前状态的影响（历史输入）。

#### **三、储备池计算的高层实现方法**

> **Reservoir Computing** 中的 **储备池** 是一个固定不变的随机网络，它会根据输入 pattern 产生一系列 **非线性动态响应**，这些响应可以看作是输入 pattern 的 **特征表示**。
>
> 输出层 **学习这些特征表示**，并通过线性分类或回归的方法来识别不同的输入模式。
>
> 这种方法利用了储备池的 **高维非线性映射** 和 **记忆效应**，从而能够高效地进行模式识别。

**示例1：语音识别任务**

假设我们使用 Reservoir Computing 来识别两种不同的语音信号：“hello”和“world”。

1. **输入阶段**：
   - 将“hello”这一段语音信号作为输入 pattern，输入到储备池中。
   - 储备池的随机连接结构会对输入信号产生复杂的动态响应，生成一系列状态序列 x(t)\mathbf{x}(t)x(t)。
2. **特征表示阶段**：
   - 储备池状态序列 x(t)\mathbf{x}(t)x(t) 会反映出“hello”这一输入 pattern 的特征。类似地，输入“world”语音信号时，储备池会产生另一组不同的状态序列。
3. **输出层训练阶段**：
   - 收集多个样本语音（“hello”和“world”），并记录对应的储备池状态序列。
   - 训练输出层，使其学会区分“hello”和“world”对应的不同状态序列。
   - 具体方法上，输出层通常使用 **线性回归** 或 **分类器（如 SVM）** 来拟合不同输入 pattern 的特征。
4. **测试与模式识别阶段**：
   - 当新的输入 pattern（如一段新的语音“hello”）进入时，储备池会再次对其产生动态响应。
   - 输出层根据储备池的状态序列判断输入 pattern 是“hello”还是“world”。

**总结**

- **储备池** 提供了对输入 pattern 的动态响应，类似于一种 **非线性特征提取**。
- **输出层** 则根据这些动态响应进行模式识别，即 **学习不同输入 pattern 对应的储备池状态特征**。

#### **四、储备池计算的优势与局限**

**储备池计算的特性**

储备池计算是动态网络的一种特殊形式，具备以下几个特性：

1. **随机连接和稀疏性**：储备池中的神经元（或节点）通常是随机连接的，且连接较为稀疏。这种随机结构有助于捕捉复杂的非线性关系。
2. **非线性动力学**：储备池内部的节点通常使用 **非线性激活函数**（如 tanh、ReLU 等），使得储备池的状态随输入变化呈现复杂的动态特性。
3. **无需训练储备池权重**：储备池中的权重初始化后不再改变，只需训练输出层的权重。这大大减少了训练复杂度。

**优势**：

- **训练效率高**：储备池内部权重不参与训练，只需训练输出层，极大降低了计算复杂度。
- **强大的非线性特征提取能力**：储备池通过高维映射捕捉到输入信号中的复杂模式。
- **良好的鲁棒性和泛化能力**：储备池的随机连接结构增强了模型的鲁棒性。

**局限**：

- **储备池设计难度**：虽然储备池内部权重不需训练，但如何选择储备池的大小、稀疏性和激活函数仍然需要大量实验调试。
- **依赖随机性**：储备池的性能依赖于随机初始化，如果初始化不佳，可能会导致模型效果不稳定。
- **无法捕捉长期依赖**：储备池通常只能捕捉到短时记忆，难以处理长时间依赖的任务。

#### 五、**体外神经网络作为储备池的计算能力体现**

当你使用 **体外神经网络（Biological Neural Network, BNNs）** 作为储备池时，它的计算能力主要体现在以下几个方面：

1. **非线性动态映射**

- 体外 BNNs 具有丰富的 **非线性动态响应特性**，它们对电刺激（如声音序列编码的刺激）会产生复杂的时空电活动。
- 每一种不同的声音序列刺激，会在 BNNs 内部激发不同的 **神经元组合和连接**，导致储备池状态（即神经元放电模式）发生变化。
- 这种复杂的动态变化，类似于在储备池计算中将输入数据映射到 **高维动态非线性空间** 中，使得不同输入序列的响应特征能够在高维空间中被展开和区分。

2. **时间相关性的记忆效应**

- 体外 BNNs 具有 **突触可塑性** 和 **网络结构的自发性活动**，这使得它能够对不同时间长度的输入信号产生 **短时记忆** 效应。
- 在声音序列刺激下，BNNs 会根据历史输入的特征逐渐调整其内部状态，表现出 **时间依赖性**，类似于储备池计算中的记忆窗口。
- 这种记忆效应使得 BNNs 能够捕捉输入序列中的 **时间相关模式**，从而为后续的模式识别提供丰富的特征表示。

3. **随机连接增强计算能力**

- 体外 BNNs 中的神经元连接是 **随机分布** 的，这种随机连接类似于 **储备池计算** 中的稀疏随机网络结构。
- 随机连接的 BNNs 对输入刺激具有 **广泛的响应特性**，不同神经元的激活模式对输入序列会产生不同的响应。这种多样性和广泛响应提升了 BNNs 的 **模式识别能力**，为后续分类提供了多维度的特征表示。

------

##### **为什么还需要使用 SVM 进行分类？**

尽管体外 BNNs 作为储备池，能够对输入序列进行非线性映射，并捕捉时间序列中的模式特征，但仍然需要使用 **支持向量机（SVM）** 或其他分类器进行最后的分类，其原因如下：

1. **线性分离能力有限**

- 体外 BNNs 作为储备池，仅负责将输入数据 **映射到高维空间**，并在其状态中记录输入的时序特征。
- 尽管 BNNs 的内部状态已经包含了输入序列的丰富特征，但这些特征在高维空间中未必能够 **线性分离**。
- SVM 可以通过寻找一个 **最佳超平面**，将高维空间中的特征进行 **线性分类**。这是储备池计算中常见的做法，即储备池负责特征提取，而分类器负责决策。

2. **简化分类模型，避免复杂训练**

- 如果直接使用体外 BNNs 的输出状态进行分类，那么需要构建一个复杂的 **非线性分类器**，这可能会增加训练的难度和计算成本。
- 使用 SVM 可以避免这种复杂的非线性分类过程，因为 SVM 擅长在 **高维空间中进行线性分类**。储备池的输出作为 SVM 的输入，能够快速、有效地进行分类。

3. **提高分类性能和泛化能力**

- SVM 作为一种 **监督学习分类器**，能够根据训练数据的标签找到最优的分类边界，因此在分类任务上具有较好的 **泛化能力**。
- 体外 BNNs 提供了丰富的、包含时序特征的高维输入，而 SVM 则利用这些高维特征来进行准确的模式识别和分类，提升分类性能。

------

**实例：声音序列识别任务中的具体流程**

假设你的实验中有 8 种不同的声音序列输入，编码为电刺激序列，分别施加在体外 BNNs 上。可以按照如下步骤执行识别任务：

1. **输入刺激**：
   - 施加 8 种不同的声音序列编码刺激，每种声音序列映射为一组电刺激脉冲序列。
   - 体外 BNNs 对每种刺激产生不同的放电响应模式。
2. **提取储备池状态**：
   - 记录体外 BNNs 在每种刺激下的神经元放电活动，构建出 **高维状态矩阵**，即每个电极的时间序列响应。
   - 这些放电活动矩阵可以看作是储备池的高维状态表示，包含了输入声音序列的时间特征和非线性变化。
3. **特征提取与输入 SVM**：
   - 将不同声音序列刺激下的 BNNs 响应数据作为 **SVM 的输入特征**。通常可以采用 **降维** 或 **特征选择** 的方法，提取关键特征。
   - 通过 SVM 训练模型，对 8 种声音序列进行分类。
4. **分类与评估**：
   - SVM 使用高维特征输入，找到最佳分类超平面，实现对 8 种声音序列的 **分类识别**。
   - 计算分类准确率，评估 BNNs 作为储备池的计算能力。

####  一些文章

- [IEICE Proceedings Series - 模块化拓扑增强了生物神经元网络中的储层计算性能](https://www.ieice.org/publications/proceedings/summary.php?iconf=NOLTA&session_num=D2L-1&number=D2L-11&year=2023)
- [大脑连接与储层计算的结合 |PLOS 计算生物学](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010639)





### 1. **电刺激扩散和非特异性激活**

- 电刺激往往会扩散，尤其是在电极之间的距离较短或信号较强的情况下，电场可以跨越多个模块。这样一来，即使刺激设置在一个电极上，周围的神经元也可能受到非特异性影响。
- 这种情况在二维培养的模块化网络中尤为明显，因为模块间的连接并没有真正的物理屏障来阻挡刺激信号的传播。

### 2. **电极位置无法与模块完全对齐**

- 模块化的设计需要在特定位置排列神经元群，但MEA2100的电极阵列通常是标准网格形式，而不是与模块完全一致的对齐方式。人为操作，刺激电极和模块间可能会产生位置偏移，使得刺激难以集中在一个模块区域。





人工神经元尽管与生物神经元有着惊人的相似性，但其行为方式并不一样。生物神经元和SNN在以下方面有根本性的不同：

- SNN只有一般的通用结构，没有特异化的结构
- 生物神经元并不直接传递冲动。为了进行交流，必须在突触间隙中交换称为神经递质的化学物质
- 计算与学习规则



SNN 是一种仿生模型，其计算单元（神经元）不直接处理连续值信号，而是依赖于尖峰发放（脉冲）来传递信息。这种行为与生物神经元一致，生物神经元通常通过动作电位（尖峰）来传递信号，而不是通过连续的电压变化。

为了让传统的输入数据（如图像、语音信号）能够被 SNN 处理，必须先将这些连续值数据转化为一系列尖峰信号。这就需要**尖峰编码机制**来完成这种转换。

尖峰神经网络的一个重要特性是 **时间依赖性（Temporal Dependency）**。尖峰信号不仅包含幅值信息，还包含时间信息。尖峰的 **发放时间和频率** 是信息的主要载体。

因此，尖峰编码不仅要将输入信号转化为尖峰序列，还需要保持或突出输入信号中的时间特性，以便 SNN 能够充分利用其时间维度进行处理。

19年的Nature, Towards spike-based machine intelligence with neuromorphic computing, [nature.com/articles/s41](http://link.zhihu.com/?target=https%3A//www.nature.com/articles/s41586-019-1677-2)





自娱自乐观点？

![image-20241117120009922](C:\Users\Yundid\AppData\Roaming\Typora\typora-user-images\image-20241117120009922.png)



定义不同的输入与脉冲频率之间的关系

后处理的方式多种多样，这也有很大的空间等待着我们去进一步研究和创新。

比如我们可以将输出层的脉冲神经元阈值设置的极高，然后输出层直接输出为膜电压就好，我们只需观察哪个神经元上的膜电压高，就判断样本是属于对应的哪一类。或者直接以脉冲序列形式输出，根据脉冲的数量判断样本的分类。

[(99+ 封私信 / 80 条消息) 脉冲神经网络(SNN) - 知乎](https://www.zhihu.com/topic/20672735/top-answers)

![image-20241117123936129](C:\Users\Yundid\AppData\Roaming\Typora\typora-user-images\image-20241117123936129.png)

**STDP本身与损失函数无直接关联，它是一种基于时序的无监督学习规则，主要通过调整神经元之间的突触权重以响应它们的相对激活时序，而不是针对减少损失函数的值。**



既然在每个小环境内都无限依赖大模型提供智能的想法都是不可能的， 我们依然需要另一些比较简单但是和任务自洽的模型， 能够恰好的嵌入到任务环境中去，并无需巨大样本或能量作为支撑。







储备池中使用随机生成的矩阵来定义循环神经网络（Recurrent Neural Network, RNN），这增加了不确定性和复杂性。

有很多超参数（metaparameters）需要调整，比如储备池的大小、激活函数等，这让优化过程变得复杂。





### 1. **明确研究的核心问题**

- **储备池模型的优化：** 基于生物神经网络(BNN)的机制设计和优化储备池模型，以提升模型的效率、适应性或任务性能。
- **模型验证：** 使用60MEA记录系统搭建体外BNN实验，验证优化后的储备池模型的生物合理性和实际效能。

需要聚焦于两个方面的逻辑关系：

- **从BNN中提取启发（生物机制 -> 模型优化）。**
- **优化的储备池模型在真实神经网络上的表现如何（模型 -> 生物验证）。**



### 2. **储备池优化与实验验证的闭环设计**

设计一个从模型到实验，再从实验到模型的闭环流程：

1. **模型构建：** 基于储备池的最新研究和生物机制设计储备池模型。
2. **实验验证：** 在MEA平台上验证模型中的假设和优化效果。
3. **反馈优化：** 根据实验结果进一步调整储备池的网络参数或动态特性。
4. **跨平台验证：** 通过不同任务（分类、预测）验证优化效果的普适性。



**背景：**

- 储备池计算是一种利用复杂动态系统的非线性动力学和短期记忆特性的机器学习方法，广泛应用于时间序列预测、分类任务等。
- 体外BNN具有天然的模块化结构、非线性动态特性和自适应性，这些特性可为储备池计算模型提供新的设计灵感。



#### **3.核心问题**

**如何将体外BNN的生物机制用于优化储备池计算模型？优化后模型在不同任务中的表现如何？**

1. **背景：**
   - 储备池计算是一种利用复杂动态系统的非线性动力学和短期记忆特性的机器学习方法，广泛应用于时间序列预测、分类任务等。
   - 体外BNN具有天然的模块化结构、非线性动态特性和自适应性，这些特性可为储备池计算模型提供新的设计灵感。
2. **关键问题：**
   - **机制提取：** 体外BNN中有哪些生物机制（如模块化结构、突触可塑性）可以用作储备池设计的理论依据？
   - **模型优化：** 如何结合体外BNN的动力学特性（如短期记忆、非线性响应），优化储备池的网络结构、神经元动态或输入编码方式？
   - **任务验证：** 优化后的储备池模型在特定任务（如时间序列预测、分类）的表现如何，是否超越传统模型？
3. **核心目标：**
   - 从体外BNN中提取启发性生物特性，优化储备池模型的设计。
   - 在多个任务场景下（分类、预测、记忆测试等）验证优化模型的性能改进。





然而，由于 FFNN 独立处理样本，**难以处理时间相关的事件或记忆需求**。

RNN 通过**递归连接**提供丰富的**非线性动态和记忆能力**，**适合处理时间序列数据**。

然而，RNN 的**训练具有挑战性**，例如梯度消失或爆炸问题、较长的训练时间以及对权重初始化的高敏感性。

**储备池计算（RC）** 是一种受生物启发的基于 RNN 的计算框架，起源于 2000 年代初的两种独立模型：回声状态网络（ESN）和液态状态机（LSM）。这些模型通过 Verstraeten 等人统一为 RC 框架，并在后续研究中得以系统化。

RC 的典型网络由储备池和读出层组成：

- 储备池是一个**随机连接的**神经网络，用于生成复杂的高维瞬态响应，作为储备池的状态。
- 读出层通过简单的**线性学习算法**（如线性回归）处理这些状态，生成输出。
- RC 的独特之处在于，**储备池的权重通常保持固定，仅训练读出层**。

优势：

- **生物学相关性**：作为神经网络的生物模型，RC被认为能够更好地解释生物神经系统的动态特性。
- **物理实现**：储备池框架逐渐应用于光学、电学等领域，表现出较好的可扩展性。
- **对深度学习的补充**：RC在应对对抗样本攻击、提高鲁棒性和适应性等方面具有独特优势，成为深度学习的有益补充。



---但是当下储备池设计依旧存在缺陷。

前馈神经网络（FFNN）-> 递归神经网络（RNN）{基于误差梯度近似的新算法（APRL）+ 反向传播去相关（BPDC） } -> 长短时记忆网络（LSTM）





### **储备池状态 X(n)是什么？**

- **X(n)**是储备池中所有神经元在第 n 个时间步的活动状态。
- 它不是储备池的最终输出，而是储备池内部动态的中间结果。
- 储备池状态通过储备池中的复杂动态（非线性递归和泄漏更新）从输入 U(n) 演化而来。

换句话说，储备池状态 X(n) 是输入 U(n) 被储备池结构处理后，在高维空间中的映射结果。这种映射结果本质上是储备池对输入的“高维投影”，它为读出层提供了丰富的信息。

这里的 x(n)是储备池的“内部状态”，它记录了当前时刻储备池对输入的响应，以及储备池的记忆效应。

**全局状态向量 X(n)：**

- 储备池中的每个神经元在某一时刻 n 都有一个状态 xi(n)，表示当前神经元的激活值。
- 储备池的整体输出是一个由所有神经元的状态组成的向量： x(n)=[x1(n),x2(n),...,xN(n)] 其中 N是储备池中神经元的总数。

**储备池的输出**是一个高维状态向量 x(n)，它由储备池中所有神经元的状态共同组成。这些状态通过递归和当前输入的动态交互生成，表示了输入信号在高维空间中的投影。

> 储备池中的每个神经元都需要将来自外部输入和内部递归反馈的激励综合起来，生成新的状态。

**W(i,j)：** 储备池内部神经元之间的递归权重，影响递归更新过程。

**Win(i,k)：** 输入信号到储备池神经元的权重矩阵，控制输入如何影响储备池。

输入信号不会直接受到递归连接的影响，但递归连接会通过储备池状态的更新**间接调制**输入的动态响应。

**递归：** 定义了储备池内部的动态特性，决定每个神经元的状态如何受到其他神经元的影响。

> 每个神经元的状态不仅与当前输入相关，也受到前一时刻其他神经元状态的影响。







### **储备池与滤波器结合的优势**

1. **分离特征，提高效率：** 通过预处理，储备池不需要同时建模趋势和周期性，降低了复杂性。
2. **增强鲁棒性：** 在数据中存在噪声或非平稳性时，HP滤波器可以减少对储备池的干扰，提升建模效果。
3. **适配多任务场景：** 不同的储备池可以专注于不同的分量，有利于同时完成多任务预测。









待调研文章：

The Cerebral Cortex: A Delay-Coupled Recurrent Oscillator Network?

> 有一篇综述回顾了大脑皮层的组织与功能进行了回顾。

Real-Time Computation at the Edge of Chaos in Recurrent Neural Networks

> 边缘混沌

A Model of Corticostriatal Plasticity for Learning Oculomotor Associations and Sequences 

> PFC

Complex sensory-motor sequence learning based on recurrent state representation and reinforcement learning

> 

The importance of mixed selectivity in complex cognitive tasks

> 混合选择性

P. F. Dominey, ‘‘Complex sensory-motor sequence learning based on
recurrent state representation and reinforcement learning,’’ Biol. Cybern.,
vol. 73, no. 3, pp. 265–274, Aug. 1995.

> **生物学启发：** 使用生物结构（PFC和纹状体）的功能作为模型灵感，但模型的目标更多是理论和应用导向，强调解决一般的感觉-运动序列问题。
>
> 前额皮层子网络（PFC） + 纹状体子网络（Striatum）

P. Dominey, M. Arbib, and J.-P. Joseph, ‘‘A model of corticostriatal
plasticity for learning oculomotor associations and sequences,’’ J. Cogn.
Neurosci., vol. 7, no. 3, pp. 311–336, Jul. 1995.

> 深入模拟灵长类眼动系统的神经机制，尤其是皮层-纹状体的突触可塑性和强化学习过程。
> 视觉输入（Visual input） + 纹状体（Striatum）

M. Rigotti, O. Barak, M. R.Warden, X.-J.Wang, N. D. Daw, E. K. Miller, and S. Fusi, ‘‘The importance of mixed selectivity in complex cognitive tasks,’’ Nature, vol. 497, no. 7451, pp. 585–590, May 2013.

> 混合选择性

C. J. Bruce and M. E. Goldberg, ‘‘Primate frontal eye fields. I. Single
neurons discharging before saccades,’’ J. Neurophysiol., vol. 53, no. 3,
pp. 603–635, Mar. 1985.

[26] P. Barone and J.-P. Joseph, ‘‘Prefrontal cortex and spatial sequencing
in macaque monkey,’’ Exp. Brain Res., vol. 78, no. 3, pp. 447–464,
Dec. 1989.

> 猕猴眼动实验

A small-world topology enhances the echo state property and signal propagation in reservoir computing

> 小世界ESN

 Hands-on reservoir computing: a tutorial for practical implementation

> 储备池入门







### 1. PPT制作基本思路

cortex 皮层

什么样的生物特性作为启发

从哪些方面去改进储备池架构





背景部分其实可以多介绍一些。

皮层前额叶-纹状体子网络开创先河。

储备池目前基础模型两个。

储备池当下的优化方法。

> DeepESN，Small World

也有直接将生物神经网络作为储备池、

引出不足，提出生物机制启发性的储备池优化方法、



技术路线？

- 生物机制

  > 雪崩状态，功能连接度，等等。





### 2. 储备池计算概述

**储备池计算源于生物学和递归神经网络的研究，初步原型由Dominey开发。**

**储备池计算（RC）** 是一种受生物启发的基于 RNN 的计算框架，起源于 2000 年代初的两种独立模型：回声状态网络（ESN）和液态状态机（LSM）。最初用于时序信号处理，是一种基于随机连接的递归神经网络。这些模型通过 Verstraeten 等人统一为 RC 框架，并在后续研究中得以系统化。

RC 的典型网络由储备池和读出层组成：

- 储备池是一个随机连接的神经网络，用于生成复杂的高维瞬态响应，作为储备池的状态。
- 读出层通过简单的线性学习算法（如线性回归）处理这些状态，生成输出。
- RC 的独特之处在于，储备池的权重通常保持固定，仅训练读出层。

储备池计算（RC）的复杂动态表明，RC不仅仅是一个机器学习框架，它还与物理学、生物学和神经科学高度相关。

疑惑：

1. 在第VI部分，通过讨论皮层-纹状体模型和耦合振荡网络，展示储备池动态所引发的高维响应如何为大脑机制提供新见解。



### 3. 储备池计算的发展历程

![](C:\Users\Yundid\AppData\Roaming\Typora\typora-user-images\image-20241121214913425.png)

#### **1. 储备池计算的历史起源**

- **早期生物学启发**
  储备池计算（Reservoir Computing, RC）源于20世纪80-90年代对生物学系统的研究。例如，研究者通过猕猴眼跳实验发现，皮层-纹状体系统的神经元对特定空间运动序列表现出选择性响应。这种“混合选择性”（mixed selectivity）的概念成为了RC的重要理论基础之一。
- **Dominey模型**
  1995年，Dominey开发了基于前额皮层（PFC）和纹状体的第一个储备池计算原型。模型采用固定的递归连接（不随训练更新），并结合奖励学习机制来完成输出。这一思想奠定了固定储备池权重的核心理念。

------

#### **2. RNN的早期研究与局限性**

- 递归神经网络（RNN）的优劣

  与仅适用于静态数据的前馈神经网络（FFNN）不同，RNN可以处理时间相关数据并表示动态系统。其早期代表包括：

  - **Hopfield网络**：以对称权重连接为特点，训练时多采用无监督学习，但容易陷入混沌或随机状态。
  - **BPTT和RTRL算法**：能够通过时间反向传播优化递归网络，但面临梯度消失、爆炸和高计算成本问题。
  - **LSTM网络**：1997年提出，通过“门机制”缓解了长时依赖问题，成为RNN中的经典架构。

------

#### **3. 储备池计算的正式形成**

- **突破性算法**
  2000年，Atiya和Parlos提出了APRL算法（基于误差梯度的递归学习），首次区分了递归网络中的“快速更新”部分（输出层权重）和“缓慢变化”部分（隐藏层权重），从而显著降低了训练复杂度。

- 两大核心模型

  21世纪初，两种独立发展的模型标志着储备池计算的正式诞生：

  - **回声状态网络（ESN）**：由Jaeger提出，通过固定的随机递归网络作为储备池，将低维输入映射到高维空间。
  - **液态状态机（LSM）**：由Maass提出，借鉴生物神经元的“液态”动态，将输入信号转化为非线性时空模式。

- 统一储备池框架

  Verstraeten等人在后续研究中将ESN、LSM等算法统一为储备池计算（RC）框架，其关键特征为：

  1. 储备池权重固定，仅需训练读出层权重。
  2. 利用储备池的高维动态状态捕获输入信号特征，简化任务建模。







### 4. 两种基本框架

#### **1. ESN**

- ### **储备池状态更新的核心概念**

  储备池状态指的是储备池中每个神经元的活动水平（通常用向量 x(n)x(n)x(n) 表示）。储备池的动态更新反映了这些神经元在接收到输入信号 u(n)u(n)u(n) 后的激活变化。公式的分两步进行：**非线性更新** 和 **泄漏更新**。

  ------

  ### **第一步：非线性更新**

  公式：

  x~(n)=tanh⁡(Winu(n)+Wx(n−1))\tilde{x}(n) = \tanh(W_{in} u(n) + W x(n-1))x~(n)=tanh(Winu(n)+Wx(n−1))

  这一步的核心作用是计算每个神经元的 **新状态（更新态）**。以下是每个部分的含义：

  1. **Winu(n)W_{in} u(n)Winu(n)：输入影响**
     - u(n)u(n)u(n) 是当前的输入信号（一个向量），例如传感器记录的时间序列数据。
     - WinW_{in}Win 是一个权重矩阵，定义了输入信号如何映射到储备池中的神经元。
     - Winu(n)W_{in} u(n)Winu(n) 表示输入信号对储备池神经元的影响。
  2. **Wx(n−1)W x(n-1)Wx(n−1)：递归影响**
     - x(n−1)x(n-1)x(n−1) 是上一时刻储备池神经元的状态向量，表示储备池中每个神经元的活动水平。
     - WWW 是储备池神经元之间的连接权重矩阵，描述了神经元之间的交互关系。
     - Wx(n−1)W x(n-1)Wx(n−1) 表示储备池内部的递归连接对当前状态的影响。
  3. **非线性激活函数 tanh⁡()\tanh()tanh()**
     - 对线性组合结果应用 tanh⁡\tanhtanh 函数，使状态变化具有非线性特性，增加储备池的动态复杂性。
     - tanh⁡\tanhtanh 的输出范围在 -1 到 1 之间，防止状态值无限增大。

  总结：这一步结合了输入信号和储备池内部递归关系，计算出一个新的神经元状态（更新态） x~(n)\tilde{x}(n)x~(n)。

  ------

  ### **第二步：泄漏更新**

  公式：

  x(n)=(1−α)x(n−1)+αx~(n)x(n) = (1 - \alpha) x(n-1) + \alpha \tilde{x}(n)x(n)=(1−α)x(n−1)+αx~(n)

  这一步的核心作用是平衡上一时刻的状态 x(n−1)x(n-1)x(n−1) 和当前的更新态 x~(n)\tilde{x}(n)x~(n)。具体含义如下：

  1. **泄漏率 α\alphaα**

     - α\alphaα 是一个超参数，取值范围在 0 到 1 之间。

     - 它控制新状态 

       x~(n)\tilde{x}(n)x~(n)

        对当前状态 

       x(n)x(n)x(n)

        的影响有多大。

       - α=1\alpha = 1α=1：完全采用新状态（快速变化）。
       - α=0\alpha = 0α=0：完全保留旧状态（无变化）。

     - 在实践中，适当选择 α\alphaα 可以平衡动态响应速度和记忆能力。

  2. **线性组合**

     - (1−α)x(n−1)(1 - \alpha) x(n-1)(1−α)x(n−1)：保留了一部分旧状态，体现了系统的记忆性。
     - αx~(n)\alpha \tilde{x}(n)αx~(n)：引入了更新态，反映了对新输入信号的响应。

  总结：这一步通过泄漏机制平滑了状态更新，既保持了对历史的记忆，又能够响应新的输入。

  ------

  ### **读出层公式**

  公式：

  y(n)=Woutx(n)y(n) = W_{out} x(n)y(n)=Woutx(n)

  - 这是储备池的输出计算公式。
  - y(n)y(n)y(n) 是模型最终的输出信号，用于解决具体任务（如时间序列预测、分类等）。
  - WoutW_{out}Wout 是唯一需要训练的权重矩阵，通过线性回归优化，使得 y(n)y(n)y(n) 尽可能接近目标信号 ytarget(n)y_{\text{target}}(n)ytarget(n)。

  ------

  ### **状态更新的直观理解**

  假设储备池是一个动态水池：

  1. **输入信号是注入水池的水流**，通过输入权重 WinW_{in}Win 决定流入的路径和速度。
  2. **储备池内部有复杂的水流交互**，由权重矩阵 WWW 控制。
  3. **泄漏机制类似于水池的排水孔**，决定水流变化的速度。泄漏率 α\alphaα 越大，反应越快；越小，水池的状态变化越缓慢。

  ------

  ### **总结**

  储备池状态的动态更新分为两步：

  1. 通过输入和内部递归计算 **更新态**。
  2. 结合历史状态和平滑机制，得到当前状态。




#### 2. LSM

**液态状态机 (Liquid State Machine, LSM)** 是由 Maass 等人提出的一种储备池计算模型，最初用于研究大脑机制和神经微电路建模。与采用非尖峰人工神经元的 **回声状态网络 (ESN)** 不同，LSM 更加贴近生物学机制，因为它基于 **尖峰神经网络 (Spiking Neural Networks, SNNs)**，并包含递归储备池结构。

------

### **LSM 的核心特点**

1. **储备池结构：**

   - 储备池由尖峰整合与发放 (Integrate-and-Fire, IF) 神经元组成，通常是三维结构，且局部连接。
   - 外部输入以尖峰序列 (spike train) 的形式刺激储备池。
   - 储备池被称为“液体”，其动态状态被比作水池表面的涟漪。

2. **动态公式：**

   - **储备池状态更新：**

     xM(t)=LMu(t)x_M(t) = L_M u(t)xM(t)=LMu(t)

     - xM(t)x_M(t)xM(t)：储备池的动态状态。
     - u(t)u(t)u(t)：输入尖峰信号。
     - LML_MLM：将输入尖峰信号映射到储备池状态的“液体过滤器”。

   - **读出层输出：**

     y(t)=fM(xM(t))y(t) = f_M(x_M(t))y(t)=fM(xM(t))

     - y(t)y(t)y(t)：输出向量。
     - fMf_MfM：“无记忆”的读出映射函数（通常是简单的线性函数）。

   - 储备池的状态通过液体过滤器转换输入，读出层使用简单算法从储备池状态中提取输出。

------

### **LSM 的核心性质**

1. **分离性质 (Separation Property, SP)：**
   - 确保不同输入 u(t)u(t)u(t) 导致不同的内部状态 xM(t)x_M(t)xM(t)。
   - 条件：储备池的液体过滤器 LML_MLM 满足逐点分离特性。
2. **近似性质 (Approximation Property, AP)：**
   - 确保读出层能根据不同的储备池状态 xM(t)x_M(t)xM(t) 生成目标输出。
   - 条件：读出层映射函数 fMf_MfM 满足近似特性。
3. **记忆能力：**
   - LSM 的 SP 和 AP 性质与 ESN 的“回声状态性质 (ESP)”和记忆能力类似，确保储备池的递归网络功能正常。

------

### **总结**

- **结构与生物学相关性：** LSM 更接近生物神经元行为，采用尖峰神经元和局部连接的三维储备池。
- **动态特性：** 储备池通过液体过滤器处理输入尖峰，生成高维动态状态。
- **核心性质：** LSM 确保分离性（不同输入产生不同状态）和近似性（读出层能近似生成目标输出）。
- **适用性：** LSM 适用于研究神经网络的动态特性，同时为机器学习提供了生物启发的计算模型。

通过这些性质，LSM 能在时间序列数据处理、模式识别等任务中表现出色，同时提供对神经科学的建模价值。



#### **对比 ESNs 和 LSMs**

1. **主要区别：**
   - ESNs（Echo State Networks）
     - 使用非尖峰神经元（non-spiking neurons）。
     - 更加灵活，适合多种模型修改。
   - LSMs（Liquid State Machines）
     - 使用尖峰积分-发放（spiking integrate-and-fire, IF）神经元。
     - 更接近生物神经网络的特性，因此更适合用于研究大脑的信息处理机制。
     - 生物启发的尖峰实现使其特别适合新型硬件（如神经形态芯片）的应用。

它们的主要区别在于：**LSM 使用尖峰积分-发放（IF）神经元**，而 **ESN 基于非尖峰神经元**。这种差异使得 LSM 更加符合生物学上的合理性，因此适用于研究大脑中的信息处理机制。

但由于 LSM 的生物启发特性以及尖峰神经元的实现，它更适合新型硬件（如神经形态芯片）[47]。因此，已有一些基于 LSM 的硬件设计和应用被报道。

相比之下，**ESN 在模型可修改性上表现更佳**。为了增强网络性能，许多 ESN 的变体被提出（见第 III 节）。这些改进主要是为了克服传统 ESN 的不足。



### **5.  储备池模型的训练**

本节深入探讨储备池计算（RC）模型的多种训练和优化方法（见表1）。除了传统的读出层训练外，还探讨了旨在从不同方面改进储备池构建的技术，从而实现更高效的模型。这些方法包括但不限于：

1. **经典读出层训练**：例如岭回归（ridge regression）[34]。

   > 岭回归在许多具体任务中表现优异，特别是当储备池提供丰富的非线性动态时。它易于实现且训练速度快，因此吸引了许多非机器学习背景的研究者。

2. **在线学习方法**：如最小均方误差法（least mean square method）和 FORCE 学习 [35]。

   > 储备池并不总是能够被良好初始化，存在一定问题。
   >
   > 解决这一问题的一种有效方法是**在线自适应学习**。在此过程中，通常引入一个反馈回路，使储备池与读出层之间形成动态适应关系。这种方法允许模型在训练过程中不断调整，适应数据分布的变化。
   >
   >
   > 最小均方误差（LMS）方法简单，计算开销低，但在应对时间依赖数据和特征值分布较宽的情况下表现不佳。
   >
   > 递归最小二乘（RLS）更加鲁棒且收敛速度快，但计算成本高（复杂度为 ( O(N^2) \），适合对精度要求较高的场景。
   >
   >
   > 不同于 ESN 避免混沌行为的设计，FORCE 学习利用混沌行为，通过调控储备池的突触强度，动态抑制这种混沌行为，将混沌转化为丰富的目标模式。
   >
   > FORCE 学习中储备池内部权重 (J) 和读出层权重 (W) 是可训练的。
   >
   > 更新机制基于改进的 RLS 方法，实现了高效的在线学习，适合动态系统中的时间序列建模。

3. **预训练**：如粒子群优化（particle swarm optimization, PSO）[52]。

4. **基于梯度的在线学习**：如反向传播算法 [75]，[76]。

   > 针对非尖峰人工神经元（如 ESN）的储备池模型，可以通过梯度下降（GD）方法进行训练。

5. **进化学习方法**：如 Evolino [66]。

   > 传统 RC 模型的性能高度依赖权重和超参数的随机初始化。
   > 使用演化算法（EA）来训练或预训练储备池，可构建名为 **神经进化（neuroevolution）** 的储备池优化方法。

6. **符合生物学原理的学习技术**：如 Hebbian 学习法 [77]。

   > 在 ESN 中，Hebbian 学习最初试图减少权重矩阵的特征值扩散，但未成功 [67]。
   > 在 LSM 中，STDP 基于 Hebbian 学习，用于改善分离特性（SP）。



#### **3. FORCE 学习的变体与实现**

尽管 FORCE 学习在动态系统建模方面表现优异，但它也存在以下不足：

- 训练后的网络过于复杂，难以分析单个神经元的活动。
- 在某些复杂的实际任务（如语音识别）中，与基于梯度的网络相比，FORCE 学习需要更多的神经元才能达到相似的性能 [56]。
- 表明 FORCE 学习虽然能够生成复杂的运动模式，但在强干扰情况下仍存在稳定性不足的问题

因此，许多研究致力于改进 FORCE 学习，涵盖了神经科学（如尖峰网络）到物理和硬件实现的广泛领域。

- **“扩展” FORCE：**
  通过使用期望输出生成网络内部每个神经元的目标值，提出了一种更通用的内部学习方法。这种被称为“目标生成”网络的方法被进一步改进 [58]，使得储备池网络可以保留时间信息，并在高噪声环境下生成复杂的高维轨迹（见图 5）。然而，这种方法在尖峰网络中的实现仍然不现实。
- **全FORCE（Full-FORCE）：**
  后来，[59] 提出了一种全FORCE算法，与传统 FORCE 学习相比，它：
  1. 需要更少的神经元；
  2. 在噪声环境下表现显著更好；
  3. 可应用于尖峰神经网络（SNNs）。
     这种算法已通过 Python 的尖峰网络框架 Nengo 实现 [60]，并适合实际操作。
- **其他改进方法：**
  1. **两步 FORCE（Two-Step FORCE）：** 收敛速度更快 [61]。
  2. **转移 FORCE（Transfer-FORCE）：** 结合了 LMS 和 RLS 的优点，提升了学习性能 [55]。
  3. **R-FORCE：** 专注于多维序列建模 [62]。

最近，[82] 开发了一个面向对象的开源 Python 包，通过 TensorFlow/Keras 接口实现了 FORCE 学习，进一步提升了其实用性。

------

### **总结**

1. **FORCE 学习的局限性：**
   - 网络复杂性高，难以分析内部神经元活动。
   - 在复杂任务（如语音识别）中需要更多的神经元以匹配基于梯度的网络性能。
2. **主要改进方向：**
   - **扩展 FORCE：** 通过生成每个神经元的目标值改进学习过程，但仍难以实现尖峰网络中的应用。
   - **全FORCE（Full-FORCE）：** 在减少神经元需求、提升噪声环境性能的同时，支持尖峰神经网络的实现。
   - **其他变体：** 包括两步 FORCE、转移 FORCE 和 R-FORCE，这些方法分别在收敛速度、多维建模和学习效率上有所突破。
3. **实用性与实现：**
   - 全FORCE 和其他变体已在 Python 框架（如 Nengo 和 TensorFlow/Keras）中实现，为实际研究和工程提供了便利。
4. **未来研究方向：**
   - 将 FORCE 学习进一步简化或适配硬件实现（如尖峰网络和神经形态计算芯片）。
   - 探索如何提升复杂任务的效率，尤其是减少网络规模的同时确保性能稳定性。



**训练相关：**

**优化空间有限**：

- 如果储备池随机初始化得不好，输出层的优化可能不够理想，储备池可能无法很好地表示输入特性。

### **3. 为什么要优化输出层之外的部分？**

传统岭回归仅优化输出层权重 WoutW_{\text{out}}Wout，假设储备池的随机权重 W,WinW, W_{\text{in}}W,Win 已经足够好。但现实中，这种假设可能不成立，导致以下问题：

1. **储备池动态表达能力不足**：
   随机初始化的储备池可能无法生成丰富的高维特征表示，从而限制了任务性能。
2. **随机初始化的不可控性**：
   储备池权重（W,WinW, W_{\text{in}}W,Win​）完全随机时，可能导致动态特性不稳定或偏离任务需求。
3. **难以适应特定任务需求**：
   储备池初始化后是固定的，很难针对不同任务进行调整。

其他方法不仅优化输出层权重，还通过动态调整储备池内部连接权重、输入权重或神经元特性，使储备池更加适应任务需求，提高整个模型的性能和鲁棒性。这些方法从不同角度扩展了储备池计算的可能性，而不仅仅局限于简单的线性输出优化。







### **B. 基于ESN的储备池计算的最新趋势**

##### **1) 多重储备池结构**

**a: 深度ESN（Deep ESN）**

传统ESN模型存在两个主要局限性：

1. **性能饱和问题**：当储备池规模增大到一定程度后，模型性能提升趋于停滞。
2. **时间尺度处理不足**：单一的大型储备池在同时处理不同时间尺度上的数据时表现较差。

为了解决上述问题，研究者开始探索具有多重时间尺度动态的储备池结构。研究表明，通过堆叠具有不同拓扑结构的递归网络，可以在不同层中生成多重时间尺度表示。基于此，2016年，[49] 提出了**深度储备池计算模型（Deep Reservoir Computing Model）**，即深度回声状态网络（DeepESN）。其结构通过堆叠多个储备池层次实现层次化的时间尺度表示（见 Fig. 6）。

###### **数学模型**

假设储备池层数为 NNN，扩展的储备池状态更新公式为：

1. **每一层的更新公式**：

   x~(i)(n)=tanh⁡(Wfwd(i)x(i−1)(n)+Wrec(i)x(i)(n−1))\tilde{x}^{(i)}(n) = \tanh\left(W^{(i)}_{\text{fwd}}x^{(i-1)}(n) + W^{(i)}_{\text{rec}}x^{(i)}(n-1)\right)x~(i)(n)=tanh(Wfwd(i)x(i−1)(n)+Wrec(i)x(i)(n−1))

   x(i)(n)=(1−α(i))x(i)(n−1)+α(i)x~(i)(n)x^{(i)}(n) = (1 - \alpha^{(i)})x^{(i)}(n-1) + \alpha^{(i)}\tilde{x}^{(i)}(n)x(i)(n)=(1−α(i))x(i)(n−1)+α(i)x~(i)(n)

   其中，x(i)(n)x^{(i)}(n)x(i)(n) 是第 iii 层储备池的状态向量，α(i)\alpha^{(i)}α(i) 是该层的泄漏率，Wfwd(i)W^{(i)}_{\text{fwd}}Wfwd(i) 是连接第 i−1i-1i−1 层与第 iii 层的权重矩阵，Wrec(i)W^{(i)}_{\text{rec}}Wrec(i) 是第 iii 层储备池的递归权重矩阵。

2. **多层状态的组合**：

   x(n)=⟨x(1)(n),x(2)(n),…,x(N)(n)⟩x(n) = \langle x^{(1)}(n), x^{(2)}(n), \ldots, x^{(N)}(n) \ranglex(n)=⟨x(1)(n),x(2)(n),…,x(N)(n)⟩

###### **实验结果与优点**

1. **多重时间尺度表示**：网络层次中的时间尺度呈现有序分布，逐层降低动态更新频率。
2. **多重频率表示**：越高的网络层次越倾向于处理较低的频率。
3. **扰动影响的延续性**：输入层的扰动对储备池上层的影响持续时间较长，而当输入被分布到所有层时，这种影响被显著削弱。

###### **关键结构特性**

通过增加网络深度并加大输入与储备池之间的距离，可以有效实现时间尺度的分离。这种特性使得深度ESN成为处理序列和时间数据的高效储备池学习算法的潜在设计方案。

###### **近期发展**

自2022年起，Tanaka等研究团队进一步改进了深度ESN架构，并结合其他技术提出了一系列新模型：

1. **序列重采样技术**（2022年）[92]。
2. **霍德里克-普雷斯科特滤波器（Hodrick-Prescott Filter）**（2023年）[93]。

这些改进模型在时间序列预测任务中表现出高预测性能，且训练成本相对较低。

------

#### **总结**

深度ESN通过堆叠多层储备池，引入了多重时间尺度动态和多重频率表示的能力。这一结构克服了传统ESN的性能饱和和时间尺度不足问题，使其在处理复杂的时间序列数据时具有显著优势。近年来的研究进一步优化了深度ESN的架构，结合了新技术后，在时间序列预测任务中实现了更高的性能和效率。这一进展为储备池计算的广泛应用提供了新的方向和可能性。



### **1. 针对性设计：人为界定时间尺度**

这种情况是通过结构设计来明确不同储备池层处理的时间尺度：

- **不同层的参数设定：** 在设计储备池时，通过调整关键参数（如泄漏率 α\alphaα、权重矩阵的谱半径 ρ\rhoρ、输入权重比例等）来人为决定每层储备池对时间尺度的敏感性。
  - **低层储备池（短期变化）：** 较低的泄漏率 α\alphaα 和小的谱半径 ρ\rhoρ，让它们快速响应输入信号的短期变化。
  - **高层储备池（长期趋势）：** 较高的泄漏率 α\alphaα 和大的谱半径 ρ\rhoρ，使它们拥有更长的记忆能力，捕捉长期变化。
- **输入分层：** 可以通过人为分配输入数据的不同时间分量（如直接使用霍德里克-普雷斯科特滤波器分解）给不同储备池层，让底层处理短期信号，上层处理长期信号。



在某些情况下，即使没有明确人为设定，不同储备池层也会在训练过程中自发表现出对不同时间尺度的偏好。这种趋势的出现主要与以下因素有关：

**递归动态和层间连接：** 深度储备池中，每一层的状态依赖于上一层的输出，而每一层又有不同的参数设置。这样的层叠结构本身可以形成一种逐渐分离的时间尺度表示。

- **底层：** 由于直接接收输入信号，倾向于快速响应短期变化。
- **高层：** 由于需要整合底层的输出信号，其动态特性更倾向于缓慢变化，逐步捕捉长期趋势。

**不同层的泄漏率（α\alphaα）和权重设置会影响它们对不同时间尺度的敏感性。**



#### **EEP FUZZY ESN (DFESN) 的提出**

在2019年，[48] 提出了一种结合模糊调节的全新深度ESN模型，称为 **Deep Fuzzy ESN (DFESN)**。该模型的主要特点是堆叠两个储备池，每个储备池的功能如下：

> 每一层的输出在传递到下一层时，会通过 **模糊聚类算法** 对输出特征进行调节或增强。

1. **第一个储备池：**
   - 用于特征提取和降维。
   - 将高维输入信号映射为更低维的特征表示，便于后续处理。
2. **第二个储备池：**
   - 负责特征增强，通过模糊聚类进一步优化特征。
   - 特征增强过程旨在提高分类效果。

#### **模糊调节的特点**

- 在 DFESN 中，后续储备池的输入源自前一个储备池的输出，并在进入第二个储备池前经过模糊聚类进行特征强化。
- 这种特征增强过程可以看作是 **逐层模糊调节（layer-wise fuzzy tuning）**，用以替代传统的反向传播算法。
- 模糊调节相比反向传播具有 **更低的计算成本**，无需进行复杂的梯度计算。

#### **效果**

- 模糊调节使输入样本在特征空间中更容易聚类。
- 这种改进提高了分类性能，特别是在需要处理复杂特征的任务中表现优异。

------

### **总结**

**Deep Fuzzy ESN (DFESN)** 是一种基于模糊调节的深度ESN模型。它通过以下方式增强了深度ESN的性能：

1. 在前一层储备池中进行特征提取和降维；
2. 在后一层通过模糊聚类进一步强化特征，从而提升分类能力。

相比传统的深度学习模型，DFESN 不再依赖反向传播算法，而是利用模糊调节实现逐层特征优化，显著降低了计算成本并提升了分类精度。

这使得 DFESN 成为一种高效、低成本的储备池计算模型，特别适用于分类任务及复杂特征的处理。

计划算法优化权重

#### **MLESN：**

- 采用 **进化算法** 优化储备池的架构（例如储备池的层数、连接权重、泄露率等）和具体的权重值。
- 优化重点在于 **储备池结构和权重的初始化**，以适应输入数据的特性。

**进化算法（EA）优化：** 利用 **粒子群优化（PSO）**，对整个储备池的架构和权重进行双层优化，从而在高维数据表示上达到最佳性能。





#### **a: 非线性函数读出（Non-linear Functions Readout）**

如前所述，单一储备池的 ESN 有时无法产生足够丰富的非线性动态。文献 [91] 提出了基于非线性函数的新方法，称为非线性 ESN（Non-linear ESN, NESN）。通过引入非线性函数代替传统的线性内部状态，成功降低了网络内部状态的规模，同时提高了动态复杂性，从而减少了计算负担。

具体来说，假设传统 ESN 的内部状态向量为 x(n)x(n)x(n)，在该方法中被替换为一个非线性函数：

xNESN=f(x)=a0+a1x+a2x2+⋯+anxn,x_\text{NESN} = f(x) = a_0 + a_1x + a_2x^2 + \cdots + a_nx^n,xNESN=f(x)=a0+a1x+a2x2+⋯+anxn,

其中 ana_nan 为常数项。

这种修改增加了非线性复杂性和学习能力，从而提高了时间序列预测的精度。此外，由于 NESN 保留了简单的结构设计，因此不需要广泛的训练、参数调优或复杂的优化过程。

然而，**非线性函数读出方法（Non-linear Functions Readout）** 的创新之处在于，它进一步增加了储备池状态的非线性复杂性。这是通过直接对储备池的内部状态向量 x(n)x(n)x(n) 应用一个非线性函数（例如多项式函数）来实现的，而不仅仅依赖神经元激活函数。这种方法：

1. 让状态的非线性复杂性进一步提升；
2. 在无需增加储备池规模的情况下增强了特征丰富性和学习能力。

### 理解上的关键点：

- **传统储备池的状态更新** 是线性输入、线性递归与非线性激活的结合，动态复杂性有限。
- **非线性函数读出** 将储备池的输出状态再引入额外的非线性处理层，等效于进一步增加了状态的表达能力。

### 换句话说：

传统储备池的非线性来源是激活函数，而非线性函数读出方法直接在储备池的状态输出上添加更多复杂的非线性变换。这种增强机制使得储备池更适合处理更复杂的任务，特别是在减少计算开销的前提下。

但具体到传统 **ESN** 的实现，储备池内部的非线性主要依赖于神经元的 **激活函数**（如 tanh⁡\tanhtanh）以及随机初始化的权重矩阵 **WWW**。在这种架构下，储备池的状态更新公式是线性加权求和与非线性激活的组合，核心动态来自权重矩阵和激活函数之间的交互。

------

#### **b: 小世界拓扑（Small-World Topology）**

小世界（Small-World, SW）网络最早由 [95] 提出。在一个常规拓扑网络中，每个节点通常与其邻近节点相连。而对于连接到随机选取的非邻近节点的情况，我们用连接概率 ppp 表示：

- p=0p = 0p=0 表示完全规则的拓扑；
- p=1p = 1p=1 表示完全随机的拓扑；
- p≈0.1p \approx 0.1p≈0.1 表示小世界拓扑（SW 拓扑）。

为进一步研究 ESN 的回声状态属性（Echo State Property, ESP）和学习性能，文献 [96] 提出了基于 SW 拓扑的 ESN（SWESN）。在该研究中，输入节点和输出节点被分隔开，储备池的连接结构采用 SW 拓扑。具体而言，连接到输入的神经元与连接到输出的神经元是不同的。

实验表明：

1. SW 拓扑使输入信号能够有效流向输出节点。
2. SW 拓扑的群集组织方式扩展了回声状态属性的范围。
3. 该方法提高了 ESN 的鲁棒性和学习性能。

小世界拓扑的引入并未改变储备池的核心运行机制，而是通过优化神经元的连接方式，使得储备池能够在相对固定的规模和计算成本下，表现出更强的处理能力和鲁棒性。这种改进非常适合实际硬件实现，尤其是在资源有限的情况下。

简单来说：

- 小世界拓扑 **既保留了规则网络的稳定性**，又引入了随机网络的灵活性，从而实现了更优的动态和学习性能。

### b2. **小世界拓扑在储备池中的应用**

在 **Small-World Echo State Network (SW-ESN)** 中，储备池的神经元连接被设计为小世界拓扑：

- 神经元之间的连接不仅局限于近邻（局部连接），还随机引入一部分长距离连接（随机连接）。
- **输入节点** 和 **输出节点** 分别独立于储备池，但储备池中神经元间的连接具有小世界特性。



**横轴（Spectral radius α\alphaα**）：

- 储备池中权重矩阵的谱半径，衡量网络的动态范围。
- α\alphaα 越大，储备池的动态可能越混乱。

- 当 α\alphaα 较小时（接近于 0），所有网络的 NRMSE 都较低，说明谱半径较小时网络动态受控，误差低。
- 随着 α\alphaα 增大，尤其是超过 1 时：
  - p=0.1p=0.1p=0.1 和 p=0.2p=0.2p=0.2（小世界网络）的误差增长速度最慢，性能最好。
  - p=1p=1p=1（随机网络）的误差增长最快，说明随机网络在较大动态范围时性能较差。

------



这两种改进方法分别从 **非线性动态** 和 **网络结构** 两个角度优化了 ESN 模型，扩展了其适用范围，同时提升了学习能力与鲁棒性。



动态系统下的储备池：

#### **元胞自动机（Cellular Automaton, CA）**

1. **定义：** 元胞自动机是一种动力系统，由一个具有特定形状的单元网格组成。每个单元通过一组预定义的更新规则和邻居单元交互，经过离散时间步的演化产生丰富的动态行为（见图 7B）[108]。

2. **CA的储备池扩展：**

   - **初步工作：** [109] 和 [103] 提出了基于CA的储备池计算模型，其中演化规则被引入以在自动机状态空间中创建一个时空体积（即储备池）。
   - **动态适用性：** CA系统被报告适合与其他离散动态系统（如布尔逻辑和符号处理）结合 [110]。

3. **近期研究：**

   - **网络架构改进：** 最近的研究扩展了CA的网络架构，包括引入更复杂的单元连接 [111], [112]。
   - **演化规则优化：** 一些研究致力于改进CA的演化规则，以增强其动态性能 [113]。
   - **关键时空模式：** [114] 提出了一种基于元胞自动机（ECA）的储备池，探索了ECA生成的关键时空模式的优点，特别是在时间序列数据中处理干扰长度方面，提出了异步调整的ECA（AT-ECA），以生成普适性的关键时空模式。

   

3) 耦合振荡器（Coupled Oscillators）

### **总结：耦合振荡器模型的核心特点**

> **耦合振荡器储备池模型**是一种使用多个振荡器组成的系统来模拟储备池计算的方式。振荡器是能够周期性改变其状态的动态系统。多个振荡器通过一定的耦合规则（如物理连接或化学反应）相互作用，形成复杂的动态行为。这样的模型可以用来作为储备池的一种实现形式，广泛应用于神经科学、化学计算和机器学习任务中。

- **丰富的动态响应：** 耦合振荡器通过自身的非线性动力学和相互耦合，提供复杂的高维状态。
- **时间序列处理：** 能够有效捕捉时间相关性，并生成与输入对应的动态模式。
- **物理实现：** 在机械、化学和生物系统中都有实现，例如 DNA 振荡器、化学反应网络和机械振荡系统。

这类模型的优点在于：它们不依赖传统的神经网络结构，而是通过动态系统的本质属性实现储备池计算的核心功能。



## 可以用于指导生物闭环控制任务

#### **图11 (B): 神经模拟与闭环控制**

- 描述：
  - 使用生物神经网络模拟 RC，用于控制车辆机器人在障碍物中导航。
  - 系统结合了 FORC 学习算法生成连贯信号输出。
- 意义：
  - 验证了生物神经网络作为储备池的潜力，尤其是在闭环控制任务中。



**非线性响应：**

**渐退记忆****(****短期记忆****)****：**

**空间信息处理**

**动态响应的积累**

**时间相关性的记忆效应**



- 到底要基于哪种网络结构进行拓展？



**a: 高维性与非线性（High-Dimensionality and Non-Linearity）** 研究表明，大脑皮层在静息状态下表现出一致且随机的高维振荡活动（即“静息活动”）[305]。大脑被认为拥有一个内在的外部世界模型（先验知识），并通过学习不断更新。当输入刺激到来时，特定特征敏感的神经元被“激活”，从而使网络的动态性崩溃为与刺激相关的亚状态（如振荡同步）。这表明，一旦储备池进入一个亚状态（如同步状态），其动态可能会选择性地针对特定刺激或生成特定的输出信号进行调整。



**皮层的动态高维活动 → 储备池的随机高维动态网络**；

**皮层到纹状体的任务映射 → 储备池输出层的线性解码**。



### 猕猴眼跳实验的核心发现

1. **皮层-纹状体系统的混合选择性（Mixed Selectivity）**：
   - 在实验中，研究者发现猕猴大脑中某些神经元对特定的运动方向、空间位置以及序列任务表现出**混合选择性**。
   - 这种混合选择性表现为神经元同时响应多种任务相关特征的非线性组合，而不仅仅是单一特征。
   - **核心启发**：大脑通过这种非线性和高维表征，能够有效编码复杂的时空信息。
2. **动态表征和记忆**：
   - 在猕猴完成眼跳任务时，大脑需要根据当前输入（如视觉刺激）和过去的动作（运动输出）的组合，动态调整下一步的运动。
   - 研究表明，这些神经元的状态形成了一个“内隐记忆”（internal memory），可以用来追踪和处理序列信息。

一个神经元可以对非线性组合后的多种信息**同时作出反应**，而不是只对某一种信息作出反应。

这种混合选择性表现为神经元同时响应多种任务相关特征的非线性组合，而不仅仅是单一特征。

**动态表征和记忆**：

- 在猕猴完成眼跳任务时，大脑需要根据当前输入（如视觉刺激）和过去的动作（运动输出）的组合，动态调整下一步的运动。
- 研究表明，这些神经元的状态形成了一个“内隐记忆”（internal memory），可以用来追踪和处理序列信息。





### -找不足 - PPT

在模型实现方面，虽然两种模型都在减少计算成本和训练时间方面表现出明显优势，但由于 LSM 的生物启发特性以及尖峰神经元的实现，它更适合新型硬件（如神经形态芯片）[47]。因此，已有一些基于 LSM 的硬件设计和应用被报道。

相比之下，**ESN 在模型可修改性上表现更佳**。为了增强网络性能，许多 ESN 的变体被提出（见第 III 节）。这些改进主要是为了克服传统 ESN 的不足。以下是 ESN 的主要两个问题：

1. **固定的连接权重限制性能**：ESN 的权重是随机初始化的，没有经过调优或优化过程。
2. **储备池规模扩大的性能饱和问题**：研究表明，随着储备池规模的增加，ESN 的性能提升会达到一个饱和点。这意味着单纯增加储备池规模并不能显著提高性能。

为了解决这些问题，研究者们专注于构建多储备池的 ESN，例如多层 ESN [48]、[49]、[50] 和并行储备池计算 [51]。与此同时，研究者们还提出了几种优化方法，通过进化算法 [50]、[52] 或基于梯度的优化技术 [37]，对网络的权重和超参数进行微调。



传统 RC 的共识观点认为，随机初始化一个 RNN 作为储备池并不是最佳解决方案，同时，简单地将线性读出层与储备池连接会限制下游响应的生成能力。研究发现，大脑皮层网络中的神经元并非随机连接，而是经过进化和发育过程形成的结构和突触连接[308]。

近年来，特别是在 ESN 和 LSM 的研究中，研究的重点主要是：

- **网络结构设计**：例如深度储备池；
- **参数优化**：如粒子群优化（PSO）；
- **训练规则的确定**：如 STDP 和 Hebbian 学习。









### - 结合一些其他的任务 闭环控制



**模式分类任务**：

- 语音数字识别（Spoken Digit Recognition）[88]。
- 波形分类（Waveform Classification）[89]。
- 手写数字图像识别（Handwritten Digit Image Recognition）[90]。

**时间序列预测任务**：

- 非线性自回归滑动平均（NARMA）时间序列任务 [53]。

**自适应滤波与控制任务**：

- 通道均衡基准任务（Channel Equalization Benchmark）[54]。

**系统近似与短期记忆任务**：

- 时序XOR任务（Temporal XOR Task）[43]。
- 短期记忆容量任务（Memory Capacity Task）[42]。

**验证方法**：

- 研究引入了多种基准任务，如语音、波形和时间序列预测任务，用于评价不同储备池模型的性能。
- 下一步研究可以探索更高效的储备池设计、更复杂任务的解决方案，以及储备池在现实硬件中的实现问题。





### - 研究内容

如 Jaeger [45] 所指出的，目前的研究多基于对输入数据的时间尺度分布或频谱的实验分析，尚未形成关于如何优化储备池设计的全面指导。

  网络结构设计：模拟模块化网络架构并引入深度储备池，小世界网络拓扑结构，以优化储备池结构。

  参数优化：借鉴生物储备池的动态重塑，引入优化算法，调整储备池内部权重

  应用突触可塑性机制动态调整储备池内部权重。

  借鉴生物神经网络时空编码方式改进输入信号映射。



未来研究方向包括：

1. 为 RC 寻找新的生物学特征类比，从而更深入地理解大脑和身体的机制。
2. **振荡和同步领域** 是一个值得进一步研究的重要方向。



**解释振荡与同步**： 通过 RC 模拟振荡行为，探索脑中振荡模式与同步现象的作用，为理解神经网络的动态行为提供了新的角度。

**更高效的 RC 模型**：

- 优化储备池设计以生成丰富的振荡和同步行为。
- 探索 ESP 的放宽条件对 RC 动力学和任务性能的影响。



### **1. 储备池动态特性的数学描述**

- **当前问题：** 储备池中的动态行为（如非线性映射能力、记忆容量和混沌边界）往往以经验方式优化，而缺乏统一的数学分析工具。

- 潜在解决方法：

  - **混沌边缘理论（Edge of Chaos）**：将储备池视为一个动态系统，研究其在混沌和有序之间的过渡区域的性质，如 Lyapunov 指数、谱半径等。

  - 信息理论分析：

    - **使用信息熵或互信息量化储备池对输入的非线性响应和信息保留能力。**
    - 衡量储备池的输入输出信号之间的可分离性。

### **2. 高维嵌入理论**

- **当前问题：** 储备池的高维非线性映射特性是其核心能力，但如何设计最佳的高维嵌入仍未形成统一理论。
- 潜在解决方法：
  - 覆盖定理（Cover's Theorem）：
    - 建立数学模型，解释储备池如何将低维不可分输入映射到高维可分区域。
    - 指导储备池的规模（神经元数量）和连接方式。
  - 最优高维结构设计：
    - 探讨储备池拓扑结构（例如小世界网络或稀疏随机网络）如何提高嵌入效率。

### **4. 生物启发的储备池优化**

- **当前问题：** 现有储备池设计大多是随机初始化，未结合神经科学的生物学特性。
- 潜在解决方法：
  - 混合选择性（Mixed Selectivity）：
    - 借鉴前额叶皮层中神经元的混合选择性机制，构建储备池的高维非线性响应特性。
  - 突触可塑性：
    - 引入类似 STDP（Spike-Timing-Dependent Plasticity）机制的学习规则，使储备池动态响应自适应优化。
  - 振荡同步：
    - 模拟生物神经网络中的耦合振荡机制，通过同步行为增强储备池的记忆容量和计算能力。

### **3. 任务依赖的储备池特性分析**

- **当前问题：** 储备池的设计未充分考虑任务需求，不同任务对储备池动态特性和记忆要求不同。

- 潜在解决方法：

  - 任务维度分析：

    - 为不同任务定义关键的动态需求（如短期记忆需求、非线性响应强度）。
    - 建立任务特定的动态特性矩阵，与储备池动态行为相匹配。

  - 时间尺度建模：

    - 使用多时间尺度动态系统（如 DeepESN 或耦合振荡器）匹配任务中的短期与长期记忆需求。



#### **4. 记忆容量与混沌边界**

- **短期记忆能力：** 储备池具备短期记忆特性，其记忆容量与动态系统的参数密切相关。
- 混沌边界：
  - 储备池在“混沌边界（edge of chaos）”附近时性能最佳。
  - 注意：Jaeger 指出此处的“混沌边界”实际指的是 **ESP 的边界**，而非真正的混沌现象。

- 结合雪崩临界态

Meanwhile, it can be found in a good deal of literature that reservoirs are claimed to work
best when they are tuned to operate at the so-called ‘‘edge of
chaos’’ [43], [44].

[43] N. Bertschinger and T. Natschläger, ‘‘Real-time computation at the edge
of chaos in recurrent neural networks,’’ Neural Comput., vol. 16, no. 7,
pp. 1413–1436, Jul. 2004.
[44] R. Legenstein and W. Maass, ‘‘Edge of chaos and prediction of compu-
tational performance for neural circuit models,’’ Neural Netw., vol. 20,
no. 3, pp. 323–334, Apr. 2007.



在《Real-Time Computation at the Edge of Chaos in Recurrent Neural Networks》中，强调了循环神经网络在"混沌边缘"（edge of chaos）状态下能够展现出最佳的计算能力。

而在《Critical dynamics arise during structured information presentation within embodied in vitro neuronal networks》中，也指出了**神经网络在接近临界状态时表现出最佳的任务性能**，特别是当系统接收到任务相关的结构化感官输入时。







### **技术路线：生物机制启发**

**a: 高维性与非线性（High-Dimensionality and Non-Linearity）** 研究表明，大脑皮层在静息状态下表现出一致且随机的高维振荡活动（即“静息活动”）[305]。大脑被认为拥有一个内在的外部世界模型（先验知识），并通过学习不断更新。当输入刺激到来时，特定特征敏感的神经元被“激活”，从而使网络的动态性崩溃为与刺激相关的亚状态（如振荡同步）。这表明，一旦储备池进入一个亚状态（如同步状态），其动态可能会选择性地针对特定刺激或生成特定的输出信号进行调整。

------

**b: 分离属性（Separation Property）** 根据上述内容，与刺激相关的亚状态通常具有：

- **更低的维度**
- **较少的方差**
- **特定的相关性结构**
- **因节点间的回响（振荡）而处于亚稳态**

这些活动都发生在一个高维状态空间中，因此，即使是线性分类器，也能将不同的输入良好地分离和分类。这与储备池计算模型（RC）的读出层原理类似。

------

**c: 消退记忆（Fading Memory）** 消退记忆指的是储备池计算中的短期记忆，储备池状态应依赖于最近输入，而不是远过去的输入。从振荡的角度看，猫视觉皮层中的实验支持了这一机制[306]。实验表明：

1. **特定刺激的信息可以在网络活动中持续近1秒，即使刺激已结束。**
2. **两个相继刺激的顺序可以在第二刺激结束后通过线性分类器正确分类，表明网络具有非线性XOR操作的能力。**
3. **刺激特征的身份分布在多个神经元（超过30个）之间，并通过放电速率向量和响应的时间相关性结构编码。**

这些证据表明，短期记忆不仅是网络的属性，也是振荡和回响的结果。

**长期记忆**则更为复杂。早期实验显示，大脑在未被扰动（睡眠）时的“默认”状态是一种由许多自我调节振荡组成的复杂系统，尤其是在丘脑-皮层系统中。这些振荡反映了清醒时的尖峰序列，并在睡眠期间被自发重播，从而实现“离线”突触修饰，这可能是形成长期记忆的途径。

然而，短期记忆之所以成为构建RC模型的必要条件，是因为储备池的随机连接通常保持固定而不进行修改。因此，RC模型通常难以形成长期记忆，因为这需要突触修饰（如通过重复播放实现）。一些学习算法（如STDP和FORCE学习）试图通过突触连接的修改实现长期记忆，这表明RC在长期记忆领域仍有发展空间。

------

**d: RC中的同步示例（Examples of Synchronization in RC）**

**多储备池结构（Multiple Reservoirs）**
在[49]中提出了深度储备池计算（Deep RC），其通过多储备池的级联构建了一个分层网络结构（详见第四节）。研究证明，深度储备池结构可以实现：

1. **多时间尺度表示**：沿网络层级有序分布。
2. **多频率表示**：更高层逐步关注更低频率。

根据阿诺德舌（Arnold Tongue）理论，如果保持较低的耦合强度（如弱突触连接或零权值连接），具有相似优先频率的神经元可以在不同频段内实现同步。这从神经振荡的角度解释了为什么多储备池能够实现这些功能，而传统ESN则无法实现。

------

#### **解读与意义**

这部分内容旨在讨论大脑神经活动与储备池计算（RC）模型的关系，并提供神经振荡如何支持RC模型核心特性的理论基础：

1. **高维性与非线性**：
   - 大脑中随机的高维振荡（静息状态）和刺激响应的同步性，揭示了储备池如何通过高维非线性映射，增强复杂时间模式的建模能力。
2. **分离属性**：
   - 振荡活动将刺激映射到不同的高维亚状态，确保不同输入的线性可分性。
3. **消退记忆**：
   - 短期记忆通过振荡回响实现，而长期记忆依赖突触修饰（如STDP或FORCE学习）。RC主要关注短期记忆，因为储备池的连接权重通常是固定的。
4. **多储备池的同步性**：
   - 深度储备池的多频率和多时间尺度表示，解释了多储备池结构的优势。

通过结合神经振荡理论和储备池计算模型，这部分内容为生物神经系统与人工神经网络之间的类比提供了深刻见解，同时也揭示了RC在未来发展的潜力和方向。





**基于生物机制的储备池优化设计**

生物启发的储备池网络结构优化

生物启发的储备池训练方法优化

体外生物神经网络实验验证平台搭建