**组织工程类脑智能复合体设计与开发**

**项目结题时间： 2024 年 12 月**

**中期检查： 2023 年 06 月**



### 1. 控制接口

控制不需要我们完成，只需要提供几个自由度的控制指令，完成矢量方向的构建即可.

控制指令，徐老师组分为八个控制指令，进行平面控制.

- 上，下，左，右，包含 45° 的另外四个方向.
- 方向 + 速度（大小）-> 转换为具体的飞机落点位置 -> 实现无人机的控制.

因此控制相关的其他我们并不需要关注，需要关注的为如何结合细胞放电模式进行控制指令的生成.



### 2. 指令映射模型的构建

稳态视觉诱发电位的特性决定，特定的屏幕亮点闪烁频率，可以引起大脑皮层固定的响应电位.

因此预先设定了 8 个视觉刺激模式，由此刺激模式引起不同响应电位，对应8个方向的控制.

具体到控制时，根据给定的视觉刺激诱发皮层响应，与预先设定的 8 个模式响应进行相关性分析，从而进行分类，或者说是对意图的解析，最终转换为无人机的运动方向.



### 3. 对于小车实验范式的启发

1. 明确小车闭环实验的重心或者说创新点在于编码方式（就是如何将原代放电与小车控制结合）.

2. 没有指标来衡量控制效果如何.

3. ssvep 的无人机控制前提是不同固定视觉刺激模式下，脑电的放电活动能呈现出差异且能重复出现.

   > 我们的刺激目前的刺激首先，是单位点的刺激，从实验结果来看，我们相同的刺激，并不能出现可重复的响应，每次响应都随机.
   >
   > 需要问一下师姐的图像刺激，给定对应图形的重复刺激，是否能够出现重复的响应，这种响应又是基于什么指标去评定的，放电率，相关性等.
   >
   > 目前需要考虑，未出现重复刺激的原因是位点过少的问题，还是单位点 + 频率变化 + 幅值变化 的刺激的响应本就是不可重复的.
   >
   > 但是结合我们的实验，即使不可重复，仍能驱使小车运动，但是智能性需要解释，对于徐老师实验而言，控制层面仅是可视化，准确率评定方面基于测试数据即可得出，我们的实验仅能根据碰撞次数来评定优化效果.

4. 如何评定准确率？

   > 以往的小车避障其实是规定好路径的，最终评定的是小车到达目标点经过的时间或者走的步数.

- 控制方式与我们不同.

  > 我们是实际上编码做的工作很简单，就是不同区域的放电差异，对应到左右轮，只要产生差异就能产生运动，比如右轮放电多一些，右轮轮速快，向左转.
  >
  >
  > 而徐老师组的控制方式时直接生成方向矢量，八个方向.
  >
  > 这种差异可能因为我们的需求不同，从结果上看，首先是控制准确率的衡量方式不同，我们只是根据碰撞次数是否减少来判断优化刺激是否有效.
  > 而直接控制方向，就像是有监督学习，从一开始就明确方向，因为方向是一开始我们基于刺激给定的，我们可以直接基于测试集得到分类准确率结果，而无人机仅仅是一个可视化过程.

- 无人机的控制不能进行碰撞实验，需要别的任务替代.

  > 由于无人机的特殊性，发生碰撞并不能够像小车一样进行回退，一次撞击，一个是会对飞机造成损坏，另一个是重新调整又是半天到一天的时间（飞机进行实验前需要进行各项校对）.
  >
  > 徐老师组做的任务实际是一个跟踪任务，地面有小车认为控制运动，无人机通过摄像头获得信息反馈，由受试者基于脑电控制无人机进行跟踪任务.
  >
  > 他们的建议是分类正确后，前进一步，否则停滞，最终判定的是到终点的量化时间.

- 错误如何校准恢复？

  > 徐老师组重新给定预设的刺激图案，直到响应出预设的刺激为止.
  >
  > **一般能到多少的准确率？**
  >
  > 我们又是如何做的错误校准恢复？发生了碰撞，给定惩罚刺激？**如何确保的惩罚刺激对于细胞而言是惩罚，如何确保的惩罚刺激可以使得细胞放电向我们预设的方向去变化**，也就是说我们希望调整，优化，奖惩，我们就一定需要明确我们的目标，或者说答案是清晰的，我们知道什么样的响应可以使得小车发生避障效果，但是现在我们是否明确什么样的刺激能够诱发什么样的响应.

- 控制指令能否简化？这样信息编码方式的自由度将更高.

- 无人机如何实现？

  > 如何我们想要实现无人机自由度的控制，那我认为就不能采取设定8个响应区域，8个刺激位点的方式，首先是相应区域与刺激区域的特异性关联问题，不一定关联的起来，会互相影响，其次就是一共50来个电极可以用，分完一个区域就是6-7个位点，这6-7个位点并不一定能完全体现出对于该刺激位点的响应模式.
  >
  > 所以其实可以参考师姐的图像分类，八个图像，对应八个指令，而剩下要做的就是首先，图像的差异在响应上应该能够特异性的体现出来，进而易于分类，该步实现后，可以引入更精确的分类算法，使得分类准确率得到提升，最终实现对于八自由度无人机的控制.
  >
  > 再之后需要考虑的就是结合我们的无人机任务（追踪或者其他），设置给定刺激的时机等等即可.
  >
  > 最重要需要解决的问题就是解释什么样的图案刺激诱发什么样的响应，该响应是否可重复的问题.
  
- 明确一下师姐的图像分类是基于什么做的分类.

- 我们能控制的只有有差异与无差异.

  > 回归到任务上，我们需要定义的是怎么样是避开，怎么样是撞上，这个意思需要捋一下，什么是避开的意思是，按照什么样的意图使小车运动，可以使其避开，就比如我直行是避开，左转是避开，右转还是避开，就是说我们需要明确一个路径方向，然后让细胞按这个路径方向走，而不是让其随机运动.

- 训练的目的是什么？

  > **我认为根据我们的目标，应该达到的训练效果是，训练了某种网络状态（基于拓扑结构等等评定），使得网络在给定某些刺激的时候更容易出现我们预期的响应，比如刺激1会对应两个区域之间的响应有差异，训练后再次给刺激1的话会出现较明显差异，使其容易控制转向.**
  >
  > **简言之就是如何量化训练效果？不要说碰撞次数.**
  >
  > 为什么训练后小车不容易发生碰撞了，改变了什么？控制了什么条件？
  >
  > 比如我就一条路线，路上有障碍物，训练可以使它记住障碍物的点，下次测试的时候可以发生转向.
  >
  > 问题是现在的实验小车的路径是随机的，他可以记住整张图的障碍物？那他是不是不应该在起始几个区域发生碰撞，为什么实验结果是仍然发生了碰撞？
  >
  >
  > 换句话说，就是深度学习网络可以使得网络训练出一组权重，使得每次输入后，输出与预期输出的差异最小， 目标函数值最小，那转换为我们这个训练任务，我们的目标如何量化，使得可以构建出这个目标函数，误差是什么，我们的预期输出是什么？ 

- 实际要解决的就两个问题

  >  一个是刺激序列与真实响应的映射问题.
  >
  > 一个就是响应数据与控制指令的关联问题.



### 4. 大课题意义

> 体外建立脑类器官智能复合体信息交互平台，在任务态下挖掘脑类器官信息处理机制；
>
> 其中血管化是促进培养成熟度提升的最为关键的环节;
>
> 国际上目前未见 3D 脑类器官-电子复合体信息交互 方面的文献，而针对 2D 神经网络的信息交互已有初步研究。
>
> 本项目将聚焦 3D 脑类器官，针对其学习机制、信号感知、与外界交互等问题，通过构建有效的动力学模型，建立 BT-IT 混合智能体的信息交互平台，进而探讨脑启发的人工智能模型。
>
>
> 
> 研究人工脑在不同任务模态、不同刺激输入下的动态特性和生物学习机制，探索刺激学习过程对人工脑在不同区域间、区域内的拓扑结构连接方式，**构建人工脑对外界信号的感知模型**，**发展其与外接设备的信息交互能力**，搭建一个能够完成多个任务的脑类器官智能复合体信息交互平台。
>
>
> 人工脑神经信号编解码、训练和脑启发模型构建针对人工脑学习机制不明的问题，研究人工脑的神经编解码，**通过已构建的神经调控方案对人工脑进行训练，赋予其对外界刺激的感知功能**，发展脑启发的智能模型。

### 5. 可调研的相关文献

> Shahaf 证明了 2D 培养的 皮质神经元具有选择性学习机制，开启了体外神经元网络信息处理机制的探索；骆清铭 团队建立了 2D 神经元网络的学习模型及动力学分析系统。Satohiro 通过局部动力学特 征实现了预测网络整体的放电活动（PNAS, 2017, 9517）。Yuan 基于单细胞电信号构建了 大尺度复杂网络，发现发育过程中网络的连接密度和强度逐渐增大（Nat Commun, 2020,  11:4854）。
>
> DiCarlo组通过构建猴脑V4神经元与深度学习卷积网络的线性映射关系实现图像分类，该方法借助深度学习编码方式揭示猴脑神经信号的编码机制[1]，基于此方法建立人工脑神经表征与外部电子系统任务模态的智能信息交互关系。
>
> 为了模拟大脑感官系统接收外界环境信息的功能，采用电刺激方式对人工脑施加刺激，Wagenaar研究团队设计并完成了不同刺激电压幅值以及频率下诱发神经元响应强度的实验，确定了最小有效诱发和最大饱和的刺激电压范围[2.3]
>
> 基于前期研究基础，推进了体外神经元网络学习记忆等信息处理机制的探究。2001年，Marom等人首先提出了体外培养皮层神经元网络学习模型，结合奖赏规则通过循环反复电刺激使得培养神经元网络逐渐掌握电刺激中携带的信息，并且刺激神经网络后在特定时间窗内达到预期响应强度的训练时间缩短，揭示了体外培养神经元具有选择学习机制[2.4]。
>
> 2007年，骆清明研究团队构建了培养海马神经元网络学习模型，其皮层学习模型显示了相似的特征，揭示了培养皮层和海马神经元网络可能遵循相同的学习规则和原理[2.5]。
>
> 此外，该研究团队设计了图像识别实验，验证了抑制网络同步爆发可提升培养神经元网络学习能力，为设计学习实验提供了新范式，同时证明了同步爆发对记忆巩固具有促进作用[2.9]。
>
> Tanaka研究团队使用碱性成纤维细胞生长因子构建体外新颖海马神经元网络模型，通过L和倒L字母训练，揭示新生神经元介导海马神经元网络增强模式分离能力[2.11]。
>
> 因此，为了达到预期实验结果以及结论的准确性，需要确定具备实验的人工脑的条件。前期文献已证明，同步爆发将抑制学习效果[2.6]。因此，较强的同步爆发以及超级爆发，均不纳入学习训练可塑性调控的模型范围内。同时对刺激响应较弱的模型无法进行诱发数据分析，也不在选择模型范围内。



### 6. 神经网络动力学模型的构建

需要与润芃师兄确认一下问题.

> 建立优化目标函数获得最优外部刺激信号策略.
>
> 研究其拓扑架构和构建神经网络动力学模型， 优化其神经调控方案.

1. 动力学模型的最终意义，目的是什么？

相同神经元在同一刺激模式下呈现不同诱发响应模式，其主要原因是人工脑的刺激响应与所处状态存在一定依赖关系。

在全通道数据所训练模型的测试结果中筛选出了基于LSTM与GRU的动力学模型作为人工脑的替代系统。

我们是不是建立**神经信号特征与各模态信息之间的映射关系模型**？

2. 优化刺激，是不是预先知道了我们想要什么样的刺激，进而我们是不是应该应该首先知道什么样的刺激可以诱发比较好的反应？

3. 确认一下优化的步骤是怎么样的?

   > 我认为就是一开始基于离线数据构建了一个预测模型，可以根据当前状态，之前状态以及刺激参数预测一个响应，基于这个预测模型呢，我们加入闭环系统，根据细胞的真实响应，结合预测模型，找到一个可以得到预期响应的刺激施加，而第二个模型在哪里体现的？找刺激？

   基于人工脑过去和当前时刻的神经活动来更好的预测未来的神经活动

   > 小车基于 spike train + 刺激时刻 使模型学习刺激序列与神经响应的输入输出关系，最后得到刺激序列与神经响应之间的映射关系模型。

4. 我们首先要构建足够精确的数学模型（即动力学模型）来刻画神经活动的演化规律，**对人工脑中复杂的神经动力学及其与刺激诱发的神经响应关系进行表征。**

   > 第二个模型的输入是什么？第一个是观测数据 + 刺激序列得到预测输出，那第二个模型呢？
   >
   > 真正在实验的时候，第一个模型输入的值是什么？是带刺激的输入嘛？

5. 然后利用上述数据集训练人工神经网络模型，单次输入为某时刻神经响应与一段时间的刺激序列，人工神经网络模型输出为此后一段时间的预测神经响应，通过构造目标函数最小化预测神经响应与真实神经响应的误差调节人工神经网络模型中的权重，来进行模型训练，使得模型学习到刺激序列与神经响应的输入输出关系，最后得到刺激序列与神经响应之间的映射关系模型。

   > 师姐那边的刺激序列是如何识别的？是不是因为手工施加的原因，可以很容易的获取的施加刺激序列的信息？

6. 这个是否完成了，基于辨识系统与当前状态得到优化刺激.
   
   
   
   ![image-20240504110057888](C:\Users\Yundid\AppData\Roaming\Typora\typora-user-images\image-20240504110057888.png)

(1.1) 基于动力学分析的神经调控建模

以电极阵列作为观测节点，采集神经电生理数据，构建线性或非线性动力学模型； 建立贝叶斯变分推断模型，推断人工脑中节点间的拓扑关系，辨识其拓扑架构，以人工脑的拓扑架构作为动力学模型的约束条件，建立优化目标函数，求解神经调控系统的输入矩阵，实现神经刺激节点的定位和调控方案优化。

> 调控后的方案，是离线生成的？然后再输入细胞进行实验？
> 我需要知道这个模型的输入输出对应关系

(1.2) 基于数据驱动的神经调控建模

数据驱动的神经调控方案无需系统辨识，而是通过采集的数据建模，并直接求解最优刺激方案，相比模型驱动的神经调控更加高效。使用分布式电极阵刺激和采集双向系统，采集在随机刺激下的人工脑神经响应，定义人工脑的期望放电状态，构建目标函数， 求解神经调控系统刺激序列。

> 这两个方法有什么区别？
>
> 具体是如何实现的？
>
> 我们又采用的哪一种方法？

- 一些关键词

> 网络动力学
>
> 神经调控方案和优化策略



![image-20240510114628484](C:\Users\Yundid\AppData\Roaming\Typora\typora-user-images\image-20240510114628484.png)

### 7. 师姐图像实验相关的问题

- 明确一下师姐的图像分类是基于什么做的分类.

- 拓扑结构辨识

> 不同的图像有不同的拓扑结构？进而实现分类？
>
> 我们引入拓扑结构分析的目的是什么?
>
> 以图像分类为例，图像分类准确率如何，再引入拓扑结构的分析后，有什么样的分析结果.
>
> 
>
> 其原因可能是人工脑不具备脑区划分的条件，且形态与拓扑连接在不同实验批次中存在较大差异。
>
> 这个问题怎么解决？

- 片上脑作为一个复杂的非线性系统，神经响应呈高度动态可变特征，难以形成稳定的诱发响应输出映射关系。

  > 这个问题师姐图像是如何解决的，既然很难构建映射关系，我们又该如何评定细胞对图像刺激的响应，进而作出分类呢？

- 师姐图像分类相关 - 需要明确映射方式

>  图像以什么样的标准做的分类？
>
> 什么样的响应就是x，什么样的响应就是y

- MCS是如何知道何时施加的刺激？师姐实验的刺激时刻是如何获取的？

  > 可能压根不涉及时刻问题，是先施加的刺激，后取的响应，人工操作.
  >
  > 需要向师姐明确.

- 采用 ±500mV、1Hz的双向电压刺激  - 图像



### 8. 小车闭环控制相关问题

- 当前片上脑构建映射模型的难点

> **此外片上脑作为一个复杂的非线性系统，神经响应呈高度动态可变特征，难以形成稳定的诱发响应输出映射关系。**
>
> 电极位点的选择需要优化一下，自主选择一个准确率不高，一个细胞状态发生变化，需要重新选择.
>
> 以训练片上脑根据控制需要产生更准确的神经响应。
>
>
> 师姐刺激位点的选择是在多大的时间跨度下完成的？是一天吗？还是先选点？

- 虚拟小车控制相关

> 跟师兄明确一下，当前所有小车刺激施加，轮速转换的方案，具体到每一个参数，因为需要知道刺激时刻.
>
> 如下两个方案我们采取的哪一个，那么是否还会有环境刺激.
>
> 考量这两种方法后，我们还会考虑距离信息嘛？

- ![image-20240504114606404](C:\Users\Yundid\AppData\Roaming\Typora\typora-user-images\image-20240504114606404.png)



![image-20240504114612799](C:\Users\Yundid\AppData\Roaming\Typora\typora-user-images\image-20240504114612799.png)

- ~~真实小车控制相关~~

> 真实的车有4个轮子，两个轮子的轮速控制如何控制？
>
> 前驱解决，就是只转换前面两个轮子的轮速，后面两个轮子由前面带动



### 9. 声音编码相关方案

- 因此本研究采用时空刺激序列编码声音信息，刺激位点编码不同声音音符，刺激时间间隔编码音符之间时间间隔，探究人工脑的培养神经网络处理复杂时空信息的能力，以实现对声音的识别与分类任务。

- 第二阶段：刺激位点选取，采用0.2Hz的低频刺激，刺激包括双相矩形电流脉冲(每相200µs，负相优先)。每个MEAs只使用59个电极进行刺激和信号记录，其中15号电极作为参考电极。为了确定适合刺激的电极和脉冲振幅，首先在刺激前自发电活动记录后，以伪随机顺序对所有59个电极分别使用3种不同的脉冲振幅(12、24和36μA)进行两次刺激。选取4个显示最清晰刺激反应，潜伏期在20到100毫秒之间的电极，用于随后的刺激方案。本实验选取的刺激位点如图所示，刺激幅值为24μA。

> 可以参考师兄选电极的方式，优化小车选电极的方式.
>
> 师兄在选择电极的时候，从开始到选完需要多长时间？

- 刺激位点选取，采用0.2Hz的低频刺激，刺激包括双相矩形电流脉冲(每相200µs，负相优先) - 声音



### 10. 随想

- 我们是一种**神经调控技术**，我们的算法模型优化的是刺激，使得该刺激诱发我们预期相应的可能性更大，所以这个刺激在实验过程中是不断变化的.
- 但是无人机控制的刺激的预先训练设定好的，在之后的实验阶段是不变的，他们引入算法模型提高的是对信号的分类准确率，并不是优化刺激，而是分类性能更好.
- 也就是两个的目的是不一样的，我们需要根据环境信息的实时反馈，生成**优化刺激**，（优化刺激都变化的），完成避障任务，通过避**障效果评估优化刺激的优劣**.
- 而他们是根据环境信息的实时反馈，生成**固定刺激**，完成追踪任务，**分类准确率基于模型数据集评估**，追踪任务只是在模型准确率足够高的情况下的一个工业化应用，或者说一个**可视化效果**.
- 而且他们的跟踪任务只需要生成方向向量即可，进而生成方向刺激，我们需要根据障碍物距离与角度信息实时编码刺激，就是我们的刺激更严格一些.



目前小车如何改进？

区别在于放电模式与行为建立映射，而不是基于个数差异建立映射

就是同样是10个spike  3 3 4 0 0 0与 1 2 2 2 2 1 的放电模式是不一样的





# 闭环刺激参数

### 环境刺激

- 何时施加.

  > 每 300ms 一次给定一次.

``` python
if self.times >= 3:    # 控制信号更新频次
```

- 施加范围.

  > 左右轮分别施加
  >
  > 左轮一个障碍物距离编码刺激，输入左轮刺激位点
  >
  > 右轮一个障碍物距离编码刺激，输入右轮刺激位点

- 刺激参数.

  > 不知道幅值哪里设置，频率应该是UI设置.

![image-20240520181143952](C:\Users\Yundid\AppData\Roaming\Typora\typora-user-images\image-20240520181143952.png)

### 惩罚刺激

- 何时施加.

  > 出现碰撞时施加惩罚刺激.

- 施加范围.

  > 左右轮分别施加.

- 刺激参数.

  > UI 设置，目前 500mv 5hz ISI 200ms

``` python
    # 从虚拟环境输出  距离值，并转化为刺激信号（距离信息的刺激），输出至刺激器进行刺激
    def update_distance(self, left, left_hit, right, right_hit, border_out, ang_dis_left, ang_dis_right, left_id_change, right_id_change):
        """
        左右最近距离、Robot是否出界
        ang_dis_left, ang_dis_right: 障碍物距离车子正前方的角度，左侧为负值，右侧为正值
        """

        self.angle_left = ang_dis_left
        self.angle_right = ang_dis_right

        if self.mode_train:  # 仅在训练模式下，进行奖惩刺激
            # if left_hit:
            #     self.stimulation.update_stimulation_left()    # 撞击到障碍物时的操作，相关信号需传输至刺激器，奖惩刺激
            #     self.save_spikes_left_right(0, 0, 1, 0)    # 保存spikes数据，便于后续分析
            # if right_hit:
            #     self.stimulation.update_stimulation_right()
            #     self.save_spikes_left_right(0, 0, 0, 1)    # 保存spikes数据，便于后续分析
            
            # 故意撞击
            # if right_hit:
            #     self.stimulation.update_stimulation_left()    # 撞击到障碍物时的操作，相关信号需传输至刺激器，奖惩刺激
            #     self.save_spikes_left_right(0, 0, 1, 0)    # 保存spikes数据，便于后续分析
            # if left_hit:
            #     self.stimulation.update_stimulation_right()
            #     self.save_spikes_left_right(0, 0, 0, 1)    # 保存spikes数据，便于后续分析

            if (left_hit or right_hit) and self.task == TASK.Obstacle_Avoidance:
                if left_hit:
                    self.stimulation.update_stimulation_left()    # 撞击到障碍物时的操作，相关信号需传输至刺激器，奖惩刺激
                elif right_hit:
                    self.stimulation.update_stimulation_right()
                self.save_spikes_left_right(0, 0, 1, 1)    # 保存spikes数据，便于后续分析
            elif (left_id_change or right_id_change) and self.task == TASK.Obstacle_Avoidance:
                if left_id_change and right_id_change:
                    self.stimulation.update_stimulation_left_right_reward()
```



### 奖励刺激

- 何时施加.

  > 左边和右边的标识符同时发生改变后，进行奖励刺激.

  ``` python
      # 从虚拟环境输出  距离值，并转化为刺激信号（距离信息的刺激），输出至刺激器进行刺激
      def update_distance(self, left, left_hit, right, right_hit, border_out, ang_dis_left, ang_dis_right, left_id_change, right_id_change):
          """
          左右最近距离、Robot是否出界
          ang_dis_left, ang_dis_right: 障碍物距离车子正前方的角度，左侧为负值，右侧为正值
          """
  
          self.angle_left = ang_dis_left
          self.angle_right = ang_dis_right
  
          if self.mode_train:  # 仅在训练模式下，进行奖惩刺激
              # if left_hit:
              #     self.stimulation.update_stimulation_left()    # 撞击到障碍物时的操作，相关信号需传输至刺激器，奖惩刺激
              #     self.save_spikes_left_right(0, 0, 1, 0)    # 保存spikes数据，便于后续分析
              # if right_hit:
              #     self.stimulation.update_stimulation_right()
              #     self.save_spikes_left_right(0, 0, 0, 1)    # 保存spikes数据，便于后续分析
              
              # 故意撞击
              # if right_hit:
              #     self.stimulation.update_stimulation_left()    # 撞击到障碍物时的操作，相关信号需传输至刺激器，奖惩刺激
              #     self.save_spikes_left_right(0, 0, 1, 0)    # 保存spikes数据，便于后续分析
              # if left_hit:
              #     self.stimulation.update_stimulation_right()
              #     self.save_spikes_left_right(0, 0, 0, 1)    # 保存spikes数据，便于后续分析
  
              if (left_hit or right_hit) and self.task == TASK.Obstacle_Avoidance:
                  if left_hit:
                      self.stimulation.update_stimulation_left()    # 撞击到障碍物时的操作，相关信号需传输至刺激器，奖惩刺激
                  elif right_hit:
                      self.stimulation.update_stimulation_right()
                  self.save_spikes_left_right(0, 0, 1, 1)    # 保存spikes数据，便于后续分析
              elif (left_id_change or right_id_change) and self.task == TASK.Obstacle_Avoidance:
                  if left_id_change and right_id_change:
                      self.stimulation.update_stimulation_left_right_reward()
  
      def update_stimulation_left_right_reward(self):
          """
          在障碍物ID变化时，进行奖励刺激，两侧同时刺激
          """
          all_ele = True
          if not all_ele:
              """同时刺激所连接的两个电极"""
              self.update_stimulation_stg1_stg2_reward(self.left_electrode_key, self.right_electrode_key, self.amp_reward, self.dur_reward)
          else:
              """同时刺激所有电极"""
              self.update_stimulation_stg1_reward_all_eles(self.amp_reward, self.dur_reward)
  ```

- 施加范围.

  > 全部电极施加.

- 刺激参数.

  > 奖励刺激：100Hz, 75mV, 100ms

``` python 
    def set_reward_sti(self):
        """
        奖励刺激：100Hz, 75mV, 100ms
        """
        left = 100;  right = 100
        l_time = 1000000 / left
        r_time = 1000000 / right    # μs，间隔时间

        amplidude = 75000.0  # μV

        # 刺激信号单元,定义100ms的信号去刺激，下一次更新即可
        left_amp = [];left_dur = []
        for i in range(int(left*0.1)):    # 100ms内的刺激次数
            # l_amplitude = Array[Int32]([1000, 1000, 0])    # 幅值 1000μV
            # l_duration = Array[UInt64]([20000, 20000, l_time])  # 200μs
            l_amplitude =[-amplidude, amplidude, 0]    # 幅值 1000μV
            l_duration = [200, 200, l_time]  # 200μs
            left_amp.extend(l_amplitude)
            left_dur.extend(l_duration)

        self.amp_reward = Array[Int32](left_amp)
        self.dur_reward = Array[UInt64](left_dur)

        print("Reward stimulus signal has been set!...")
```

