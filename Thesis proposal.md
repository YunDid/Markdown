# **体外培养神经网络闭环训练调控及智能行为增强研究**



### 1 文献调研相关

1. [计时是神经网络的内在属性：来自体内和体外实验的证据 |英国皇家学会哲学汇刊 B：生物科学](https://royalsocietypublishing.org/doi/full/10.1098/rstb.2012.0460)

> 生物科学 二区 这篇文章中有体外的描述 14年

看一下盲源分离技术，从这个角度看一下细胞究竟是如何理解外部刺激的，

2. [Experimental validation of the free-energy principle with in vitro neural networks | Nature Communications](https://www.nature.com/articles/s41467-023-40141-z)

> NC 一区 自由能原理的描述
>
> 可以看一下都有做什么实验，分析了什么数据？

3. [A primer on in vitro biological neural networks | OpenReview](https://openreview.net/forum?id=RdFI2ogt1j)

> 体外生物神经网络入门 

4. [3D 工程神经元培养物的网络动力学：体外电生理学的新实验模型 |科学报告](https://www.nature.com/articles/srep05489)

> 一篇3D神经网络的文章，可能与帮助

5. [In Vitro Studies of Neuronal Networks and Synaptic Plasticity in Invertebrates and in Mammals Using Multielectrode Arrays - Massobrio - 2015 - Neural Plasticity - Wiley Online Library](https://onlinelibrary.wiley.com/doi/full/10.1155/2015/196195)

> 四区 神经元图可参考

6. https://www.x-mol.com/paper/1697107062972764160/t?adv=

> **[体外神经元网络内的结构化信息呈现过程中出现关键动态](https://www.x-mol.com/paperRedirect/1697107062972764160)**
>
> 我们证明，当神经网络接收到与任务相关的结构化感官输入时，就会出现临界动态，从而将系统重新组织到接近临界状态。

7. Critical dynamics arise during structured information presentation within embodied in vitro neuronal networks.

   > NC 2023
   >
   > 实验结果表明，当神经网络接收到与任务相关的结构化感官输入时，系统会自组织到一个**接近临界的状态**。
   >
   > 神经系统可能在临界状态下优化信息传递和处理，临界状态的**典型特征**
   >
   > 因此，有研究者提出临界性可能是皮层网络自组织的一种设定点。然而，一些理论研究指出，**临界状态只在复杂的认知任务中带来益处**，而对于静息状态的网络而言，这种动态性并不重要。
   >
   > 此外，健康成人在执行工作记忆和认知任务时，响应时间的波动也呈现幂律分布，这进一步支持了临界状态对认知活动的潜在重要性。
   >
   > 大脑中的神经网络是否在自发活动时就表现出临界状态，还是只有在处理结构化信息（如认知任务）时才出现？

### 2 研究背景与现状

**1 现有体外BNNs与机器结合系统的局限性 + 缺乏智能行为的挖掘**

**生物脑-机混合智能**结合了生物大脑和机器智能各自的优势，未来有望于借助生物智能对复杂环境的感知能力，达到强于机器智能的精确性和高效性。目前，体外培养的神经网络（BNNs）已被广泛应用于生机混合系统中，例如机器人的基本运动控制和路径规划等场景。然而，大多数系统的研究重心放在**基础的神经-机器接口搭建**上，例如实现神经网络控制简单的运动指令或反射性行为，而忽略了对**智能行为提升**的探索。现有的研究仅限于**实现信号通路连接**，建立**基本的感知-反馈通路**。这些系统在实际应用中往往只满足“完成任务”的要求，而非“优化任务表现”，因此难以适应复杂的环境或具有更高要求的任务场景。缺乏对BNNs在动态任务中的表现及其智能提升的深入挖掘

- **缺乏系统性训练手段**：现有训练方法往往不能长期有效，难以在BNNs中实现高度稳定的智能行为。
- **缺乏对刺激反馈的优化**：由于神经调节剂等生理条件在体外环境中不易复现，传统的随机刺激难以帮助BNNs逐步塑造稳定的智能行为。

**2 研究意义**

BNNs 天然具备自组织和突触可塑性等特性，在理论上有可能实现自适应和学习。然而，现有的BNNs+机器系统还没有充分利用这些特性，仅作为简单的“信号转换器”来执行机械性的任务，忽略了BNNs在适应性和决策性任务中的潜在优势。为此，研究如何在现有BNNs与机器结合系统中引入智能行为的提升机制成为了一个具有重大意义的方向。

**3 研究必要性**

BNNs 天然具备自组织和突触可塑性等特性，在理论上有可能实现自适应和学习。针对目前仅限于“搭建通路”的生机混合系统，我们的研究将进一步探索如何通过**闭环训练和自适应（动态）反馈**来**提高BNNs在控制任务中的智能表现**，推动生机混合系统从基础控制阶段迈向智能行为阶段。智能行为的提升不仅有助于增强系统在复杂任务环境中的表现，也能为未来更高层次的生物控制应用奠定基础。

**4 研究目的 or 研究内容**

- 基于闭环动态反馈训练策略，实现智能行为的提升。

- 闭环策略训练下需要真正明确的应该是训练后，网络出现了什么状态使得任务执行效率更高（如何评价网络状态，有什么指标，pong中有一个临界态评估），建立一套训练效果评估体系。

  > **临界状态**是一种动态平衡状态，网络在此状态下最有效地传输和存储信息，同时能够优化响应的灵敏度和适应性。大脑的临界状态通常出现在需要处理复杂信息或做出决策的任务中，这种状态能提升其认知和学习能力。
  >
  > **且临界状态更可能由结构化信息输入或环境反馈驱动**，使神经网络调整到最佳信息处理状态。

### 3 技术路线

#### 1. **探究基础的刺激模式以及网络的基础响应模式 - 	监督学习方法去探究**

> 目前已经探究了基础的单点刺激的诱发响应强度变化，但是这种刺激应用于闭环等其他任务时，会因为过于频繁而导致细胞响应呈现不规律的变化，因此此处探究的是可用于闭环高频的基础刺激模式，以便用作刺激池中刺激模式的生成。

- **依赖点**：首先了解BNNs对基础刺激的自然响应模式。可以通过初步实验观察其自发活动、对不同频率和强度的刺激响应（目前的梯度实验基本是单点刺激，考虑MEA电流刺激的方式，闭环的频率疲劳问题，这个刺激如何设置才能最大程度的影响BNNs的网络变化）等，这样可以为训练策略的基线设定提供数据。

- **设计依据**：基于这些基础反应模式，以及综述中提到的非线性计算能，保持训练的稳定性。

- **做不同刺激参数的组合实验**

  > 设计多样化的训练策略需要测试不同的刺激参数组合（如强度、频率、空间位置等），以**筛选出适合的刺激模式**。
  >
  > 通过组合实验来确定BNNs在不同参数下的最佳反应模式。这样的组合测试希望**找到一套系统的刺激方案**，用于在训练中优化网络表现，这个系统的刺激方案会用作之后刺激池的准备。
  >
  > 参考pingpong奖惩反馈在设置的时候是一个固定的，基于预实验选择的幅值频率固定的刺激，只是施加范围有时随机。

#### 2. **编解码码策略的设计**

- **依赖点**：编码策略是训练的核心，需要确保编码后的刺激能够最大程度的反应现实信息，并且能够被BNNs感知，这个感知在于温和感知，例如小车控制的频率过高，任务的复杂度，其实一定程度上很难评估真实的网络状态，至少让细胞保证温和可正常响应的状态
- 这部分虽然课题三中并不是我们完成，但是这部分很大程度也会影响训练效果，至少需要有一套基本的可供训练测试用的编解码策略。

#### 3. **设定特定行为目标与反馈机制**

- **依赖点**：明确BNNs在训练中的具体行为目标，目标越具体，训练策略的设计就越具针对性。
- **设计依据**：监督学习中到底该基于什么目标去设定响应强度阈值，强化学习基于什么任务进行奖惩，何时进行奖惩，什么刺激形式（电，光，化学）。

#### 4. **考量神经网络的各个突触可塑性机制以及网络结构进行训练设计**

- **依赖点**：突触可塑性（如长期增强或短期突触调节）这种生理基础。设计训练策略时，需要充分利用和激发BNNs的突触可塑性，以便使其逐步形成稳定的学习模式。

- **设计依据**：可以利用重复刺激和周期性训练，如课题三中提到的时间依赖刺激，**使BNNs的突触发生持久性调整**。通过观察网络的突触变化来调节训练强度或频率，以促进突触在学习过程中逐步加强。

  > 这个突触可塑性，突触的变化如何观测？人工构建的网络结构是否能真实反映细胞级的突触变化？

#### 5. **总体是基于闭环系统的动态调整**

- **依赖点**：BNNs的状态是动态变化的，因此训练策略也应具备灵活性，能够根据网络的实时反馈动态调整训练参数。
- **设计依据**：在实验中观察BNNs的状态变化，动态调整刺激的类型、频率和强度，位置。目前的闭环系统基本完备，对该平台复现基本可开发的程度后，进行闭环任务的探究。

有效的训练策略设计是建立在以上提到的几点，可能接触到后续的这种依赖有助于根据BNNs的具体特性和实验目标设计有针对性的训练方法，从而在最大程度上提升BNNs的智能行为表现。

- 具体路线

第一步我其实想基于闭环反馈式的训练，使得网络对于特定的刺激有预期的响应，做一定程度的调控。

> 刺激的范围是不是有要求，考虑到MEA的电流刺激方式，这个范围的临界态是什么？

第二步在未明确机制的情况的下，基于自适应奖惩做任务训练的尝试

......

### 3.1 训练方法

想要基于两种方式对体外神经网络（BNNs）尝试进行训练，看是否能够达到预期效果

监督学习 + 强化学习训练BNNS，训练方式和反馈机制：

#### 1. **监督学习 (Supervised Learning)**

- **训练方式**：在监督学习中，BNNs的每次输出行为都与预期的响应进行比较（即目标输出）。训练过程中，会提供一个明确的响应强度阈值，如果BNNs的神经活动被逐步调整，实际输出与目标之间的误差将随着训练而减少。

- **反馈机制**：监督学习依赖于精确的误差反馈。这个误差反馈目前基于什么来界定误差？，设定误差触发负反馈，让网络逐步调整其行为以符合目标方向。。

  > 前期的不涉及复杂任务，仅仅根据响应强度等简单可测量的信息进行误差反馈，后期如果任务是让BNNs控制微纳鱼朝一个特定方向游动，每次偏离目标方向角度设定误差都会触发负反馈，让网络逐步调整其行为以符合目标方向。
  >
  > **实现倒是不容易，需要把闭环数据分析功能从 MetaBOC中抽离出来，仅用作闭环监督学习实验，不然使用 metaBOC 因功能太多冗余导致，没有侧重点，实验用起来也麻烦，还得开发针对这个实验的功能**

#### 2. **强化学习 (Reinforcement Learning)**

- **训练方式**：在强化学习中，BNNs不会有固定的目标输出，而是根据动作的结果进行学习。训练过程会给出奖励或惩罚刺激，以引导BNNs调整行为来增加正反馈（奖励）并减少负反馈（惩罚）。

- **反馈机制**：强化学习的反馈基于行为的**整体结果**，而不是即时的误差。

  > 例如，在避障任务中，当BNNs成功避开障碍物时，它们会收到正反馈；如果与障碍物发生碰撞，则会收到负反馈。通过不断试错，BNNs逐渐学会避开障碍，理想希望是习得一条路线。
  >
  > 其实我并不想基于小车做强化学习的尝试，小车的不确定性太高，而且训练难度会更大，目前小车的多轮次实验也并没有出现明显的效果提升，而是因为刺激造成的相应通道信号波动呈现一定规律的变化，进而使得控制效果提升。我想复现一个类似pong游戏的简单游戏，**“有明确的正确与错误行为”**（鉴于先前游戏开发实习经历以及在脑虎实习的经历，这个的开发成本其实不高，可能也就一周左右，最长算到一个月时间也足够用于之后的实验），而且训练成本相较于小车避障任务也将更容易，将该游戏接入体外BNNs的训练中，由此构建基于游戏任务的反馈机制（奖惩将更容易设置）。

- **要点**：强化学习适合**没有固定答案、需要探索**的场景，特别是面对不确定环境时，通过奖励机制逐渐优化策略。

#### 3. **自适应奖惩刺激方法**

- **训练方式**：自适应奖惩刺激方法是一种**较灵活且迭代**的策略，不直接依赖于对BNNs内部机制的全面理解。通过反馈**逐步调整刺激选择概率**，在机制不完全明了的情况下进行训练。允许BNNs在无须详细阐明内部运作机制的情况下逐步优化功能。

  

**表格对比，PPT一页**

| 监督学习                 | 特点         | 强化学习                           |
| ------------------------ | ------------ | ---------------------------------- |
| 明确的目标值和误差反馈   | **反馈类型** | 奖励或惩罚反馈                     |
| 减少输出与目标值的差距   | **目标**     | 增加长期奖励，优化行为策略         |
| 定向控制、特定行为的训练 | **适用任务** | 探索性任务、不确定环境中的行为优化 |
| 学习与预期值的差距       | **学习过程** | 通过试错法优化长期行为表现         |

- **监督学习**：BNNs被要求控制微纳鱼游向指定方向，每次偏离都会触发负反馈，BNNs逐步校正输出，以符合预期方向。
- **强化学习**：BNNs在障碍物周围探索最优路径，每次成功避开障碍物获得奖励，随着试错次数增加，BNNs逐步学会自适应避障策略。



### 4 研究困难： 

这一领域面临的主要挑战包括：

- **缺乏多种神经调节因子**：由于体外BNN仅由神经元组成，缺乏多种神经调节因子的参与，因此将动物的训练方法移植到BNN的训练中变得困难，这增加了训练的难度。
- **生物智能的局限性**：虽然猴子可以被训练成骑自行车，但对于需要更高层次思维过程的任务（如下围棋），体外BNN很难达到类似水平。
- **稳定的通信**：稳定的通信是训练的基础，但由于我们尚未明确BNN中神经元间和网络内通信的正确编码机制，因此找到合适的通信方式仍然具有挑战性。



### **6 Idea Points**

1. 智能的聚焦点可能还得回到学习记忆机制上？

   BNNs的响应模式

   刺激信号基于突触结构的传递？进而引发特定局部的反应模式？这种突触结构是否会被破坏呢？

   如何调整突触的强度？

> - 体外神经网络只有神经元通过突触连接构成的网络结构，所以智能聚焦于网络，探索一下关于训练前后网络的临界态有什么变化，网络变化的原因是什么？突触连接的变化？那么突触会不会被破坏？对刺激的要求会有限定。
> - 体外BNNs尽管没有完整的调节机制，但它们仍然能够展示出神经元间的**自组织行为**、**突触可塑性**和某些程度的学习与记忆。通过研究这些简单的神经网络，可以揭示出**神经元间如何进行信息传递**、**如何通过突触连接形成复杂的动力学模式**。这是理解更大规模神经系统的基础。

3. 如果在解码方法中加入神经网络算法，是一种创新吗？

   解码策略上还能改嘛？只能解码spike？

> 目前的分析可能还得聚焦于 spike 分析，具体的响应模式可以引入算法进行一下分类划分。

4. **神经动力学？**

> 这个问题需要有些时间好好了解一下，因为这个可能是之后闭环训练的基础。
>
> 一直提动力学特征，这个特征到底是什么

5. 我想要从形态结构上去研究在训练前后的变化？

> 形态该基于什么方法去研究？

6. 刺激调控方式其实也可以优化

   > 光刺激（例如通过光遗传学技术）可以更精确地调控特定神经元的活动，从而引发不同的反应模式。



### 7  Questions



**动物的神经调节机制是什么？**

**机器学习的控制模型**：体外BNNs训练通常通过机器学习方法学习合适的刺激序列，以控制神经网络的行为。这包括设计反馈机制，在检测到不期望的网络活动时施加特定的反馈信号，以引导网络向所需状态收敛

**不可预测的电刺激**：当BNNs在任务中表现不佳时，施加随机或不可预测的电刺激作为负反馈。例如，在训练BNNs玩“Pong”游戏时，每当“球拍”未能接住球，研究人员会施加随机的电刺激，促使BNNs调整其活动以避免再次出现错误。 

**特定模式的电刺激**：根据BNNs的表现，施加预先设定的电刺激模式作为负反馈。这种方法需要在实验前设计好负反馈刺激的模式，并在BNNs表现不符合预期时施加，以引导其朝正确方向调整。

**化学刺激**：除了电刺激，还可能使用化学物质（如神经递质或其类似物）来施加负反馈。通过改变培养基中化学物质的浓度，影响BNNs的活动，从而实现负反馈效果。



### 8 创新点

> 除了训练方法之外的可尝试的创新

### 1. **特定任务依赖的网络结构重组**

**创新点**：研究如何通过任务导向的训练，诱导网络形成更高效的拓扑结构（探究临界态是否能够评估效果，并看是否能找到类似临界态的评估方法）。与仅关注神经元活动不同，可以研究在特定任务下网络拓扑如何自我调整，逐步发展出高度分化的模块化结构，从而提升复杂任务的执行效率。

**实现方法**：设计任务情境，观察特定结构（例如模块化的子网络）是否在任务执行中被激活或形成。如果发现特定任务依赖某些网络模块，则可以进一步通过有针对性的训练策略来增强这些模块的功能（60位点够吗）。

设计的这个任务要足够简单（太复杂感觉变量太多，困难系数更高，尤其做小车真实的控制训练上），侧重点放在网络变化上，走之前课题四评估体系。

怎么评价网络拓扑？还是从60位点建立拓扑结构？

### 2. **探索从“噪声”中提取信息的能力**

**创新点**：探索体外神经网络在高噪声环境下的智能行为表现。训练神经网络从噪声中提取有用信号，是否可以增强其对复杂环境的适应能力。

**实现方法**：在闭环训练中引入高背景噪声，观测网络如何过滤无关噪声并专注于任务相关的信号。通过调整噪声水平，逐步提升网络对低信噪比环境的适应性。

### 3. **体外BNNs的“自主”信息处理能力 智能基础**

**创新点**：研究神经网络在完全自主的情况下能否生成智能行为，减少外部干预。例如，在无任务或无明确刺激的条件下，观察其自发的突触重组和模式形成。

**实现方法**：相当于更换任务场景，创建一个无特定任务的闭环环境，通过监测自发活动的变化，分析网络是否会自发形成类似于“探索”或“试错”的行为模式。探索其内部是否存在一种“自我组织”的智能基础。

**Pingpong：**

**条件1（刺激反馈）**：当神经网络表现符合期望时给予可预测的刺激，表现不符合期望时则施加不可预测的刺激。

**条件2（静默反馈）**：与刺激反馈条件不同，静默反馈不提供任何刺激，而是用等长时间段的静默代替。

**条件3（无反馈）**：在这种条件下，改变了游戏环境，移除了乒乓板“错失”球的可能性。乒乓板未能拦截到球时，球会弹回并继续游戏，而不是触发游戏重置。

**实验结果**：

- **静默反馈条件**：静默条件下，神经网络在任务中的表现显著提升，接近临界状态的特征更加明显。相比于刺激和无反馈条件，静默条件下的临界偏离系数（DCC）显著更低（p < 0.005）。
- **刺激反馈与无反馈的对比**：在游戏表现（H/M比例 命中/未命中）方面，刺激反馈和静默反馈条件都显著优于无反馈条件（p < 0.0005和p < 0.005）。
- **无反馈条件**：尽管无反馈条件下网络也表现出一定的临界动态特征，但游戏表现明显下降，与静息状态相比没有显著差异（p = 0.085）。

![image-20241106140843488](C:\Users\Yundid\AppData\Roaming\Typora\typora-user-images\image-20241106140843488.png)





### 1. **是否可尝试 微观层面的智能行为增强：动态突触调控**

**创新点**：在神经网络的局部突触连接层面，通过精细的闭环控制实现特定突触连接的塑性变化。例如，不仅观察整个网络的活动，还要对关键突触连接进行个性化的刺激调节，培养出特定的功能性连接。这可以涉及动态监测和调节突触强度，以便在执行特定任务时，增强某些特定通路的活性。

**实现方法**：在闭环反馈过程中，监测局部突触反应，并通过自适应算法将“奖励”刺激更集中地施加在关键突触区域，观察这种局部调控对整体智能行为的影响。

**能实验中观测细胞的状态变化吗？**

### 2. **是否可尝试 引入多模态输入**

**创新点**：模拟更真实的多感官输入，以探索不同感官信息融合在智能行为中的作用。多数研究仅采用单一的电刺激或单一类型的输入，而实际的生物系统往往依赖多种感官信息的协同。

**实现方法**：设计多模式输入通道，例如将光、电、化学刺激结合在一个闭环训练系统中。

**光刺激**可以精确地针对特定的神经元群体或区域进行**特异性刺激**

**电刺激**可以有效激活更大范围的神经元或网络结构，但是空间精确度低。

1. **光电结合**是否可以充分利用光刺激的**高选择性**和电刺激的**能量效率**。例如，通过光来精确选择目标区域，再配合电刺激激发整个神经网络，从而实现更灵活、精细的控制。
2. 光电结合，是否可以在输入上编码不同的信息。例如，使用电刺激传递主要的时序信号（如快速的脉冲频率，固定幅值），同时用光信号提供更复杂的调制信息（如光的强度或频率），以便更细腻地影响神经元群体的行为。
3. 通过交替使用光和电刺激，是否可以减少单一刺激造成的“疲劳”，保持神经网络的活跃度？

**这个光电平台搭建就够呛了，保证不了精确的双模块同时刺激输入**

### 3. **是否可尝试 基于网络“疲劳”状态的学习与记忆增强**

**创新点**：研究体外神经网络在“疲劳”状态下的学习与记忆能力。当前研究大多集中在网络的活跃状态，而“疲劳”状态下的神经活动可能会反映出一些特殊的学习机制，例如巩固记忆或优化资源分配。

**实现方法**：在训练过程中，故意引入高频刺激，让网络进入疲劳状态，并观察这种状态下的突触可塑性变化。分析网络是否会在疲劳状态中优先形成更稳固的长时程突触连接，从而提升任务记忆的持久性。

### 4. 引入 ANN 作解码分析？

### 5. 是否需要结合模块化，如何结合模块化？MEA60 精度是否在模块化后，执行复杂效率的性能会降低？





### Critical dynamics arise during structured information presentation within embodied in vitro neuronal networks.

> NC 2023
>
> 实验结果表明，当神经网络接收到与任务相关的结构化感官输入时，系统会自组织到一个**接近临界的状态**。
>
> 神经系统可能在临界状态下优化信息传递和处理。
>
> 临界状态的**典型特征**
>
> 因此，有研究者提出临界性可能是皮层网络自组织的一种设定点。然而，一些理论研究指出，**临界状态只在复杂的认知任务中带来益处**，而对于静息状态的网络而言，这种动态性并不重要。
>
> 此外，健康成人在执行工作记忆和认知任务时，响应时间的波动也呈现幂律分布，这进一步支持了临界状态对认知活动的潜在重要性。
>
> 大脑中的神经网络是否在自发活动时就表现出临界状态，还是只有在处理结构化信息（如认知任务）时才出现？

1 核心在于**通过检测神经网络在不同条件下的临界性指标，验证其是否接近临界状态**。这部分实验表明，当体外皮层网络在任务状态下，其活动更接近临界状态，且临界性指标与任务表现密切相关。这表明，临界动态可能与神经网络的任务处理能力直接相关，接近临界状态的网络在执行任务时表现更为优越。

2 在任务状态下，运动和感官神经元子群同样表现出接近临界系统的特性。

> 这部分研究表明，在任务状态下，运动和感官子群神经元同样会继承整体神经网络的临界特征。这意味着，即使在特定子群中，神经元的活动在任务中也呈现出接近临界状态的特征，从而提升了任务表现。

3 实验结果表明，**反馈机制在任务中的重要性**，尤其是在临界状态下能进一步提升神经网络的信息处理能力。反馈允许神经网络根据行为结果不断调整和优化，缺乏反馈会让网络失去学习和自我优化的驱动力，因此即使网络接近临界状态，虽然接近临界状态有助于信息处理，但在缺乏关于先前行为后果的反馈信息时，单纯的临界性不足以使神经网络实现学习和记忆。

4 简而言之，这段内容表明不同神经元培养物在静息状态下的爆发模式会影响其临界性指标，这种模式的多样性提示了使用多种神经元文化进行实验的必要性，以获得全面的结果。



### 其他文章启发

早期实验包括对动物神经元的多电极刺激，通过混沌理论的应用来管理**神经元的混沌电响应**，**并尝试从中得出简单的计算结果**。





# 储备池计算相关

#### **一、储备池计算（Reservoir Computing, RC）的起源**

储备池计算（RC）最早起源于 **2000 年**，由两种不同但类似的神经网络模型分别提出：**Echo State Network (ESN)** 和 **Liquid State Machine (LSM)**。这两种模型提出了 **不同于传统循环神经网络（RNN）** 的新思路，旨在解决 RNN 中的 **梯度消失和梯度爆炸问题**，并简化训练过程。

- **传统循环神经网络的挑战**：

  - 训练循环神经网络时需要对整个网络进行反向传播，这往往会导致 **梯度消失问题**，尤其是在处理长时间序列时，模型难以捕捉到输入信号中的长时记忆。
  - 训练过程复杂，计算开销大，不适合实时任务。

- **RC 的核心思想**：

  - **Cover 定理** ：在高维空间中，随机投影的数据点更容易被线性分离。储备池计算通过非线性映射和高维投影，能够 **揭示时间序列中隐藏的模式和特性**。

  - 储备池的主要任务是对输入信号进行 **非线性动态映射**，将其转换为 **高维动态状态**，从而捕捉到输入信号中的**复杂模式和时间特征**。

    > RC 提出了一种新颖的架构设计：将循环网络分为 **储备池（Reservoir）** 和 **输出层（Readout Layer）**。储备池的内部连接是**随机初始化并保持权重不变**的，只训练一个简单的 **线性输出层**（readout layer），极大简化了计算复杂度。
  
  ![image-20241113222740160](C:\Users\Yundid\AppData\Roaming\Typora\typora-user-images\image-20241113222740160.png)

#### **二、储备池计算的理论基础**

储备池计算的理论基础来源于 **高维动态系统** 和 **统计学习理论**，主要包含以下几部分：

1. **高维映射（High-dimensional Mapping）**：
   - 输入信号经过储备池的随机连接后，被映射到一个高维空间。高维空间中，原本低维空间中复杂的非线性模式会被展开，使得后续的线性分类或回归变得更加容易。
   - 这种高维映射类似于 **支持向量机（SVM）** 中的核技巧，通过非线性映射将数据变得线性可分。
2. **回声状态特性（Echo State Property, ESP）**：
   - 在 **Echo State Network (ESN)** 中，储备池需要满足回声状态特性，即输入信号的影响会随着时间逐渐衰减，不会无限传播。这保证了储备池状态的稳定性，使其能够对当前输入和近期输入进行短时记忆。
3. **液体状态特性（Liquid State Property, LSP）**：
   - 在 **Liquid State Machine (LSM)** 中，储备池由尖峰神经网络（Spiking Neural Network, SNN）构成，模拟大脑中神经元的尖峰发放特性。输入信号进入储备池后，会像石头丢入水中一样，引发一系列复杂的动态响应，这种涌现的瞬态状态记录了输入信号的时间特征。

- **SVM vs RC**

| 特性             | 支持向量机（SVM）                  | 储备池计算（RC）                         |
| ---------------- | ---------------------------------- | ---------------------------------------- |
| **高维映射方式** | 显式或隐式使用核函数进行映射       | 通过储备池的随机连接和非线性响应实现映射 |
| **映射性质**     | 静态映射，无时间动态特性           | 动态映射，具有时间记忆和非线性特性       |
| **训练方式**     | 二次规划求解，计算复杂度高         | 只训练输出层，计算复杂度低               |
| **适用场景**     | 静态分类任务，如图像识别、文本分类 | 时序任务，如语音识别、时间序列预测       |
| **理论基础**     | 最大间隔原则、Mercer 定理          | 回声状态特性、液体状态特性               |

- **Echo State Network (ESN) Vs Liquid State Machine (LSM) **

| 特性           | Echo State Network (ESN)   | Liquid State Machine (LSM)         |
| -------------- | -------------------------- | ---------------------------------- |
| **网络类型**   | 循环神经网络 (RNN)         | 尖峰神经网络 (SNN)                 |
| **储备池权重** | 随机初始化，不参与训练     | 随机初始化，不参与训练             |
| **储备池状态** | 连续值（神经元激活值）     | 离散尖峰发放序列                   |
| **动态特性**   | 回声状态特性，具有短时记忆 | 液体状态特性，尖峰活动捕捉时间特征 |
| **应用场景**   | 时间序列预测、动态系统建模 | 神经科学建模、语音处理、机器人控制 |
| **计算效率**   | 计算相对简单               | 计算复杂度高，需要模拟尖峰发放     |

4. 基础概念

- **非线性空间** 指的是输入数据在经过**非线性变换（如激活函数）后**，不能简单地用线性关系表示。

- **线性（Linear）** 一般表示的是一种 **简单的关系**，这种关系满足 **叠加性** 和 **比例性** 两个特性。在几何上，线性关系通常表示为一条 **直线**，在高维空间中，线性关系可以表示为 **超平面**。

- **线性可分（Linearly Separable）** 是指在给定的数据集中，不同类别的数据点可以被一条 **直线**（在二维空间中）或一个 **超平面**（在高维空间中）完全分开。与数据本身是否具有线性属性无关。

- **动态网络（Dynamic Network）** 是指一种在 **时间上不断变化** 的网络结构，其输出不仅依赖于当前的输入，还依赖于过去的输入和网络的历史状态。简而言之，动态网络具备 **记忆** 和 **反馈** 特性。与静态网络不同，动态网络具有 **时间依赖性**，可以在 **时序数据** 中学习和捕捉时间相关的模式。

> 摆钟（动态系统），当你推一下摆钟时，它会开始摆动。这种摆动不仅取决于你推的力度（输入），还受到摆钟之前状态的影响（历史输入）。

#### **三、储备池计算的高层实现方法**

> **Reservoir Computing** 中的 **储备池** 是一个固定不变的随机网络，它会根据输入 pattern 产生一系列 **非线性动态响应**，这些响应可以看作是输入 pattern 的 **特征表示**。
>
> 输出层 **学习这些特征表示**，并通过线性分类或回归的方法来识别不同的输入模式。
>
> 这种方法利用了储备池的 **高维非线性映射** 和 **记忆效应**，从而能够高效地进行模式识别。

**示例1：语音识别任务**

假设我们使用 Reservoir Computing 来识别两种不同的语音信号：“hello”和“world”。

1. **输入阶段**：
   - 将“hello”这一段语音信号作为输入 pattern，输入到储备池中。
   - 储备池的随机连接结构会对输入信号产生复杂的动态响应，生成一系列状态序列 x(t)\mathbf{x}(t)x(t)。
2. **特征表示阶段**：
   - 储备池状态序列 x(t)\mathbf{x}(t)x(t) 会反映出“hello”这一输入 pattern 的特征。类似地，输入“world”语音信号时，储备池会产生另一组不同的状态序列。
3. **输出层训练阶段**：
   - 收集多个样本语音（“hello”和“world”），并记录对应的储备池状态序列。
   - 训练输出层，使其学会区分“hello”和“world”对应的不同状态序列。
   - 具体方法上，输出层通常使用 **线性回归** 或 **分类器（如 SVM）** 来拟合不同输入 pattern 的特征。
4. **测试与模式识别阶段**：
   - 当新的输入 pattern（如一段新的语音“hello”）进入时，储备池会再次对其产生动态响应。
   - 输出层根据储备池的状态序列判断输入 pattern 是“hello”还是“world”。

**总结**

- **储备池** 提供了对输入 pattern 的动态响应，类似于一种 **非线性特征提取**。
- **输出层** 则根据这些动态响应进行模式识别，即 **学习不同输入 pattern 对应的储备池状态特征**。

#### **四、储备池计算的优势与局限**

**储备池计算的特性**

储备池计算是动态网络的一种特殊形式，具备以下几个特性：

1. **随机连接和稀疏性**：储备池中的神经元（或节点）通常是随机连接的，且连接较为稀疏。这种随机结构有助于捕捉复杂的非线性关系。
2. **非线性动力学**：储备池内部的节点通常使用 **非线性激活函数**（如 tanh、ReLU 等），使得储备池的状态随输入变化呈现复杂的动态特性。
3. **无需训练储备池权重**：储备池中的权重初始化后不再改变，只需训练输出层的权重。这大大减少了训练复杂度。

**优势**：

- **训练效率高**：储备池内部权重不参与训练，只需训练输出层，极大降低了计算复杂度。
- **强大的非线性特征提取能力**：储备池通过高维映射捕捉到输入信号中的复杂模式。
- **良好的鲁棒性和泛化能力**：储备池的随机连接结构增强了模型的鲁棒性。

**局限**：

- **储备池设计难度**：虽然储备池内部权重不需训练，但如何选择储备池的大小、稀疏性和激活函数仍然需要大量实验调试。
- **依赖随机性**：储备池的性能依赖于随机初始化，如果初始化不佳，可能会导致模型效果不稳定。
- **无法捕捉长期依赖**：储备池通常只能捕捉到短时记忆，难以处理长时间依赖的任务。

#### 五、**体外神经网络作为储备池的计算能力体现**

当你使用 **体外神经网络（Biological Neural Network, BNNs）** 作为储备池时，它的计算能力主要体现在以下几个方面：

1. **非线性动态映射**

- 体外 BNNs 具有丰富的 **非线性动态响应特性**，它们对电刺激（如声音序列编码的刺激）会产生复杂的时空电活动。
- 每一种不同的声音序列刺激，会在 BNNs 内部激发不同的 **神经元组合和连接**，导致储备池状态（即神经元放电模式）发生变化。
- 这种复杂的动态变化，类似于在储备池计算中将输入数据映射到 **高维动态非线性空间** 中，使得不同输入序列的响应特征能够在高维空间中被展开和区分。

2. **时间相关性的记忆效应**

- 体外 BNNs 具有 **突触可塑性** 和 **网络结构的自发性活动**，这使得它能够对不同时间长度的输入信号产生 **短时记忆** 效应。
- 在声音序列刺激下，BNNs 会根据历史输入的特征逐渐调整其内部状态，表现出 **时间依赖性**，类似于储备池计算中的记忆窗口。
- 这种记忆效应使得 BNNs 能够捕捉输入序列中的 **时间相关模式**，从而为后续的模式识别提供丰富的特征表示。

3. **随机连接增强计算能力**

- 体外 BNNs 中的神经元连接是 **随机分布** 的，这种随机连接类似于 **储备池计算** 中的稀疏随机网络结构。
- 随机连接的 BNNs 对输入刺激具有 **广泛的响应特性**，不同神经元的激活模式对输入序列会产生不同的响应。这种多样性和广泛响应提升了 BNNs 的 **模式识别能力**，为后续分类提供了多维度的特征表示。

------

##### **为什么还需要使用 SVM 进行分类？**

尽管体外 BNNs 作为储备池，能够对输入序列进行非线性映射，并捕捉时间序列中的模式特征，但仍然需要使用 **支持向量机（SVM）** 或其他分类器进行最后的分类，其原因如下：

1. **线性分离能力有限**

- 体外 BNNs 作为储备池，仅负责将输入数据 **映射到高维空间**，并在其状态中记录输入的时序特征。
- 尽管 BNNs 的内部状态已经包含了输入序列的丰富特征，但这些特征在高维空间中未必能够 **线性分离**。
- SVM 可以通过寻找一个 **最佳超平面**，将高维空间中的特征进行 **线性分类**。这是储备池计算中常见的做法，即储备池负责特征提取，而分类器负责决策。

2. **简化分类模型，避免复杂训练**

- 如果直接使用体外 BNNs 的输出状态进行分类，那么需要构建一个复杂的 **非线性分类器**，这可能会增加训练的难度和计算成本。
- 使用 SVM 可以避免这种复杂的非线性分类过程，因为 SVM 擅长在 **高维空间中进行线性分类**。储备池的输出作为 SVM 的输入，能够快速、有效地进行分类。

3. **提高分类性能和泛化能力**

- SVM 作为一种 **监督学习分类器**，能够根据训练数据的标签找到最优的分类边界，因此在分类任务上具有较好的 **泛化能力**。
- 体外 BNNs 提供了丰富的、包含时序特征的高维输入，而 SVM 则利用这些高维特征来进行准确的模式识别和分类，提升分类性能。

------

**实例：声音序列识别任务中的具体流程**

假设你的实验中有 8 种不同的声音序列输入，编码为电刺激序列，分别施加在体外 BNNs 上。可以按照如下步骤执行识别任务：

1. **输入刺激**：
   - 施加 8 种不同的声音序列编码刺激，每种声音序列映射为一组电刺激脉冲序列。
   - 体外 BNNs 对每种刺激产生不同的放电响应模式。
2. **提取储备池状态**：
   - 记录体外 BNNs 在每种刺激下的神经元放电活动，构建出 **高维状态矩阵**，即每个电极的时间序列响应。
   - 这些放电活动矩阵可以看作是储备池的高维状态表示，包含了输入声音序列的时间特征和非线性变化。
3. **特征提取与输入 SVM**：
   - 将不同声音序列刺激下的 BNNs 响应数据作为 **SVM 的输入特征**。通常可以采用 **降维** 或 **特征选择** 的方法，提取关键特征。
   - 通过 SVM 训练模型，对 8 种声音序列进行分类。
4. **分类与评估**：
   - SVM 使用高维特征输入，找到最佳分类超平面，实现对 8 种声音序列的 **分类识别**。
   - 计算分类准确率，评估 BNNs 作为储备池的计算能力。

####  一些文章

- [IEICE Proceedings Series - 模块化拓扑增强了生物神经元网络中的储层计算性能](https://www.ieice.org/publications/proceedings/summary.php?iconf=NOLTA&session_num=D2L-1&number=D2L-11&year=2023)
- [大脑连接与储层计算的结合 |PLOS 计算生物学](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010639)





### 1. **电刺激扩散和非特异性激活**

- 电刺激往往会扩散，尤其是在电极之间的距离较短或信号较强的情况下，电场可以跨越多个模块。这样一来，即使刺激设置在一个电极上，周围的神经元也可能受到非特异性影响。
- 这种情况在二维培养的模块化网络中尤为明显，因为模块间的连接并没有真正的物理屏障来阻挡刺激信号的传播。

### 2. **电极位置无法与模块完全对齐**

- 模块化的设计需要在特定位置排列神经元群，但MEA2100的电极阵列通常是标准网格形式，而不是与模块完全一致的对齐方式。人为操作，刺激电极和模块间可能会产生位置偏移，使得刺激难以集中在一个模块区域。



改变异质性？



混沌图理解

### 每个元素的含义

混淆矩阵中的每个元素（位于行和列的交点处）表示 **特定实际发声者被预测为某一发声者的比例或频次**。更具体地：

- **对角线元素**：矩阵中的对角线元素（从左上到右下）表示 **模型正确分类的数量**。例如，如果行 1、列 1 的元素有较深颜色，表示发声者 1 的样本大多数被正确预测为发声者 1。
- **非对角线元素**：非对角线的元素表示 **错误分类的数量**。例如，如果行 1、列 2 的元素有较深颜色，表示实际是发声者 1 的样本被错误分类为发声者 2。

### 如何解读颜色深浅

- 颜色越深

   表示该元素的数值越大，即该分类情况出现得越频繁。

  - 对角线上的深色表示模型在该类的正确识别率较高。
  - 非对角线上的深色表示模型在该类的误分类率较高。





多尺度化储备池计算框架 
        作者开发了一种多尺度储备池计算框架，用于在无模型的情况下学习噪声诱导的转变。该方法的灵感来自于研究发现储备池中的超参数决定了储备池动力学的时间尺度。鉴于多尺度时间序列，可以调节超参数以匹配慢时间尺度的动力学。当储备池通过拟合输出层矩阵捕捉到慢时间尺度的动力学后，可以将快时间尺度的序列分离为一个噪声分布。在预测阶段，作者利用训练好的储备池计算机来模拟慢时间尺度的动力学，然后将从分离的噪声分布中采样的噪声（对于白噪声）或从第二个储备池中学习的噪声（对于彩色噪声）加回。整个过程在时间点上迭代，称为滚动预测。值得注意的是，当前方法与之前将噪声仅视为干扰因素的方法有所不同。
————————————————

                            版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。

原文链接：https://blog.csdn.net/weixin_54335478/article/details/133040843





物理储备池计算的实现架构分为四大类，包括时延耦合储备池（Delay-coupled RC）、动态器件储备池（Dynamic devices RC）、材料内计算储备池（In materia RC）、旋转神经元储备池（Rotating neurons RC），这些架构定义了储备池层中物理节点的拓扑连接关系。国内外研究团队尝试各种不同物理节点在这些架构下实现储备池，包括忆阻器、自旋电子器件、铁电场效应晶体管、纳米线网络等，用于模拟神经元的非线性和动态特性。

![image-20241114142857140](C:\Users\Yundid\AppData\Roaming\Typora\typora-user-images\image-20241114142857140.png)

原文链接[集成电路学院唐建石、吴华强团队发表物理储备池计算研究综述-清华大学微纳加工中心](http://tnf.ime.tsinghua.edu.cn/info/1333/1772.htm)





人工神经元尽管与生物神经元有着惊人的相似性，但其行为方式并不一样。生物神经元和SNN在以下方面有根本性的不同：

- SNN只有一般的通用结构，没有特异化的结构
- 生物神经元并不直接传递冲动。为了进行交流，必须在突触间隙中交换称为神经递质的化学物质
- 计算与学习规则



SNN 是一种仿生模型，其计算单元（神经元）不直接处理连续值信号，而是依赖于尖峰发放（脉冲）来传递信息。这种行为与生物神经元一致，生物神经元通常通过动作电位（尖峰）来传递信号，而不是通过连续的电压变化。

为了让传统的输入数据（如图像、语音信号）能够被 SNN 处理，必须先将这些连续值数据转化为一系列尖峰信号。这就需要**尖峰编码机制**来完成这种转换。

尖峰神经网络的一个重要特性是 **时间依赖性（Temporal Dependency）**。尖峰信号不仅包含幅值信息，还包含时间信息。尖峰的 **发放时间和频率** 是信息的主要载体。

因此，尖峰编码不仅要将输入信号转化为尖峰序列，还需要保持或突出输入信号中的时间特性，以便 SNN 能够充分利用其时间维度进行处理。

19年的Nature, Towards spike-based machine intelligence with neuromorphic computing, [nature.com/articles/s41](http://link.zhihu.com/?target=https%3A//www.nature.com/articles/s41586-019-1677-2)





自娱自乐观点？

![image-20241117120009922](C:\Users\Yundid\AppData\Roaming\Typora\typora-user-images\image-20241117120009922.png)



定义不同的输入与脉冲频率之间的关系

后处理的方式多种多样，这也有很大的空间等待着我们去进一步研究和创新。

比如我们可以将输出层的脉冲神经元阈值设置的极高，然后输出层直接输出为膜电压就好，我们只需观察哪个神经元上的膜电压高，就判断样本是属于对应的哪一类。或者直接以脉冲序列形式输出，根据脉冲的数量判断样本的分类。

[(99+ 封私信 / 80 条消息) 脉冲神经网络(SNN) - 知乎](https://www.zhihu.com/topic/20672735/top-answers)

![image-20241117123936129](C:\Users\Yundid\AppData\Roaming\Typora\typora-user-images\image-20241117123936129.png)

**STDP本身与损失函数无直接关联，它是一种基于时序的无监督学习规则，主要通过调整神经元之间的突触权重以响应它们的相对激活时序，而不是针对减少损失函数的值。**



既然在每个小环境内都无限依赖大模型提供智能的想法都是不可能的， 我们依然需要另一些比较简单但是和任务自洽的模型， 能够恰好的嵌入到任务环境中去，并无需巨大样本或能量作为支撑。







储备池中使用随机生成的矩阵来定义循环神经网络（Recurrent Neural Network, RNN），这增加了不确定性和复杂性。

有很多超参数（metaparameters）需要调整，比如储备池的大小、激活函数等，这让优化过程变得复杂。





### 1. **明确研究的核心问题**

基于你的描述，核心问题可以分为以下两部分：

- **储备池模型的优化：** 基于生物神经网络(BNN)的机制设计和优化储备池模型，以提升模型的效率、适应性或任务性能。
- **模型验证：** 使用60MEA记录系统搭建体外BNN实验，验证优化后的储备池模型的生物合理性和实际效能。

你需要聚焦于这两个方面的逻辑关系：

- **从BNN中提取启发（生物机制 -> 模型优化）。**
- **优化的储备池模型在真实神经网络上的表现如何（模型 -> 生物验证）。**



### 2. **储备池优化与实验验证的闭环设计**

建议设计一个从模型到实验，再从实验到模型的闭环流程：

1. **模型构建：** 基于储备池的最新研究和生物机制设计储备池模型。
2. **实验验证：** 在MEA平台上验证模型中的假设和优化效果。
3. **反馈优化：** 根据实验结果进一步调整储备池的网络参数或动态特性。
4. **跨平台验证：** 通过不同任务（分类、预测）验证优化效果的普适性。



**背景：**

- 储备池计算是一种利用复杂动态系统的非线性动力学和短期记忆特性的机器学习方法，广泛应用于时间序列预测、分类任务等。
- 体外BNN具有天然的模块化结构、非线性动态特性和自适应性，这些特性可为储备池计算模型提供新的设计灵感。



#### **3.核心问题**

**如何将体外BNN的生物机制用于优化储备池计算模型？优化后模型在不同任务中的表现如何？**

1. **背景：**
   - 储备池计算是一种利用复杂动态系统的非线性动力学和短期记忆特性的机器学习方法，广泛应用于时间序列预测、分类任务等。
   - 体外BNN具有天然的模块化结构、非线性动态特性和自适应性，这些特性可为储备池计算模型提供新的设计灵感。
2. **关键问题：**
   - **机制提取：** 体外BNN中有哪些生物机制（如模块化结构、突触可塑性）可以用作储备池设计的理论依据？
   - **模型优化：** 如何结合体外BNN的动力学特性（如短期记忆、非线性响应），优化储备池的网络结构、神经元动态或输入编码方式？
   - **任务验证：** 优化后的储备池模型在特定任务（如时间序列预测、分类）的表现如何，是否超越传统模型？
3. **核心目标：**
   - 从体外BNN中提取启发性生物特性，优化储备池模型的设计。
   - 在多个任务场景下（分类、预测、记忆测试等）验证优化模型的性能改进。

