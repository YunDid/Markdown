### 1. 如何最大化硕士期间的收益。

### 2. 建立结果，文章评价体系。

### 3. 由投资机构去找风口。

### 4. 计算机作为一项基本技能，去做另外一个领域的深度挖掘。

### 5. 生理特征

对数据的哪些生理特征进行分析。

### 6. Wrong

文章结论并不一定正确，需要看所发的期刊。



### 沈自所讨论

期望观察到的网络功能连接性的变化将因为重复的时空激活模式而偏向于某种特定状态。

如何评价网络功能连接性？

突触可塑性。

图拓扑结构变化：包括平均节点度、网络密度、网络权重以及小世界属性等指标，评估训练前后网络连接 结构及功能的变化。

特定刺激诱发的时空刺激模式不同。



#### 智能提升机制到底是什么，与之前没有一体化的机制，与人类的，都有什么机制和区别

> 回答思路：目标 + 措施（训练） + 生物机理

一体化智能提升机制：该机制目的为提升类生命一体化机器人在动作以及任务场景下的运动效能，通过引入训练（高低频，强直刺激，时间延迟刺激，网络分区训练（多脑区，自定义模块化结构），加药改善网络），并根据实时训练效果施加奖惩刺激，以诱导片上脑生理结构以及功能网络向预期状态变化，例如片上脑网络**功能连接性**的变化将因为**重复的时空激活模式**而偏向于某种特定状态，进而在感知环境传感输入时，有更可控的响应信号。

**1. 生物机理：**（PPT一部分）

赫布可塑性

LTP - LTD

STDP - SRDP

**2. 功能网络：**（PPT一部分）

图拓扑结构变化：包括平均节点度、网络密度、网络权重以及小世界属性等指标，评估训练前后网络连接结构及功能的变化。

**3.与之前控制小车有什么机制区别 or 技术难点**（PPT一部分）

> 回答思路：可能更多的论述不在与生物本身的机制区别，可能是一种机制，针对不同的任务，不同的外接控制设备，涉及到的训练难度以及方法难度不同

1. 反馈的环境信息不同，一体化类生命机器人传感信息相较于普通小车的距离传感信息可包含的信息更多，更细微，课题四虽不涉及编码，但是越复杂的编码信息，片上脑的响应情况的与功能变化更难评估（如太高频刺激下的细胞会更快衰弱，训练时间受限），对训练的方法要求更高。

2. 控制指令层面，小车控制仅包含方向信号，按响应强度比例进行方向向量的控制，但是类生命控制包含肌肉组织的（目前肌肉控制不明，可以讨论明确一下片上脑->肌肉->机器人的具体控制流）各项复杂控制，将具有更高的控制自由度，简单理解将训练细胞适应更多的功能状态。

   **就以上提到的两个问题，与类生命机器人一体化后，编码的刺激以及解码的指令将包含更多，更高维度的信息，意味着一体化类生命机器人的训练需要有更多的训练刺激参数的探索以及多训练方法的整合（多脑区 + 强直 + 加药）以完成更多信息的非线性转化，对神经元的向预期状态训练的控制要求更高。**

3. 目前训练能达到的效果为

   1）开环（不根据反馈实时调整） - 图像 七分类 80%  声音 5分类 80% 开环的训练难度更低，组合强直刺激以及模块化方法即可依照体外神经元的自身的非线性计算能力实现分类任务

   2)  闭环（带反馈+调整） 机器人避障（2-20hz梯度的环境刺激 避开与碰撞的固定奖惩刺激 自由度少（只控制左右两轮轮速））10min训练有较好的避障效果。

   效果其实不好说，包括追踪与抓取，目前的效果都一般

**4.技术难点？**





四个层级？

肌肉 动作 协同 肌肉

从机电层面进行控制指令分类的决策

片上脑层面仅是训练信号

监督学习层面 ground truth

任务级别

持续学习 新的学习是否会冲淡旧的学习效果

如何量化表示出两者的对比？自行车？





多层级上去解释机制

分子细胞 动力学 



定义一些区域 不同区域有不同的含义

在运动过程中 动力系统的模型 区域响应之间有什么影响 响应与动作又是什么关系？

那些区域施加那些刺激

基于先验知识功能分区（存在动力学特性） 动力学关键节点



左动作 右任务

反哺神经科学上的机制问题

#### 智能提升机制到底是什么，与之前没有一体化的机制，与人类的，都有什么机制和区别

> 回答思路：
>
> 围绕两个点，从多层级（动作 + 任务）上回答 
>
> 1. 功能性HUB节点，对于最终监督学习任务，结合拓扑结构变化并对应HUB节点，该节点对最终监督任务贡献更大
> 2. 基于先验知识的功能分区，预实验定义多个功能分区，例如2个（记录区域），该区域与运动的协同更强烈，由该区域的响应控制肌肉，神经信号 -> 直接转成四条肌肉的刺激信号。

PPT

左基于动作 -> 训到能控制肌肉向指定状态收缩

右基于任务 -> 结合任务控制一体化机器人完成指定任务



#### STDP 参数探索 时间依赖可塑性 如何应用到训练中？



#### 自适应算法的基本原理，如何根据误差实现自适应

自适应算法的目标在于通过闭环系统，基于性能的实时反馈不断发现和完善有效的训练序列，逐渐塑造功能连接性。

在训练中，我们的自适应算法用于调整奖惩刺激序列的模式，预先设定了刺激池，即频率与幅值在一定范围内的刺激序列，每一种刺激序、列标定被选中的概率，初始化相等。在监督学习中，设置两种运动方向并建立与记录电极区域放电情况的关联，（笼统说方向，具体四块肌肉怎么控制不好明确）

当类生命机器人运动方向出现与预期方向的误差时（角度评估），施加自适应惩罚刺激，施加刺激后观测机器人运动方向是否改善，改善则提高该刺激下次被选中概率，反之降低（意图在意通过重复且有效的惩罚刺激，调控网络状态向预期状态发展），当类生命机器人的运动方向无误差时，施加奖励刺激（奖励刺激依旧从刺激池中随机选取，意图在于不改变当前网络状态）。逐步诱导可塑性，以达到期望的性能。



